# -*- coding: utf-8 -*-
"""
a2_ocr_mixed_text_table_GPT.py
---------------------------------------
ƒê·ªçc v√† x·ª≠ l√Ω t√†i li·ªáu h·ªón h·ª£p (vƒÉn b·∫£n + b·∫£ng/·∫£nh):
- T·ª± nh·∫≠n d·∫°ng lo·∫°i file: DOCX, PDF, TXT, h√¨nh scan.
- OCR khi c·∫ßn (kh√¥ng l∆∞u .png, x·ª≠ l√Ω trong RAM).
- GPT ch·ªâ ch·ªânh s·ª≠a ph·∫ßn b·∫£ng ho·∫∑c ·∫£nh.
- Xu·∫•t file text + meta JSON (metadata m·ªü r·ªông cho vector store).

Gi·ªØ nguy√™n t√≠nh nƒÉng c≈© + TH√äM fallback PDF scan (rasterize to√†n trang).
"""

import os, re, io, json, hashlib, argparse
from pathlib import Path
from typing import List, Dict, Optional

import fitz  # PyMuPDF
import pdfplumber
from PIL import Image
import pytesseract
from docx import Document
from tqdm import tqdm
from langdetect import detect
import pandas as pd
import yaml

# ===== GPT enhancer (an to√†n c√≥ fallback) =====
try:
    # n·∫øu ch·∫°y ki·ªÉu: python -m src.a1_ocr_mixed_text_table_GPT
    from src.gpt_enhancer import enhance_with_gpt as _enhance_with_gpt
except Exception:
    try:
        # n·∫øu file ƒëang trong c√πng package
        from .gpt_enhancer import enhance_with_gpt as _enhance_with_gpt
    except Exception:
        # fallback: kh√¥ng d√πng GPT, tr·∫£ nguy√™n vƒÉn ƒë·ªÉ kh√¥ng b·ªã NameError
        def _enhance_with_gpt(text, meta=None, image=None, **kwargs):
            return text

# ========== C·∫•u h√¨nh m·∫∑c ƒë·ªãnh (linh ho·∫°t & portable) ==========
BASE_DIR = Path(__file__).resolve().parent.parent

SAVE_PNG_DEBUG = False
OCR_LANG = "vie+eng"
OCR_CFG = "--psm 6 preserve_interword_spaces=1"

# ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi (ƒë·∫£m b·∫£o nh·∫•t cho pipeline c·ª•c b·ªô)
INPUT_DIR_ABS  = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\inputs\c_financial_reports_test"
OUTPUT_DIR_ABS = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\a1_ocr_mixed_text_table_GPT_test so sanh p1a"
# S·ª≠a l·ªói thi·∫øu d·∫•u c√°ch ·ªü '3. ChatBot_project' n·∫øu ng∆∞·ªùi d√πng l·ª° c·∫•u h√¨nh sai
YAML_RULE_PATH_ABS = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\configs\a1_text_only_rules.yaml"

# Fallback n·∫øu ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi kh√¥ng t·ªìn t·∫°i
INPUT_DIR  = INPUT_DIR_ABS if os.path.exists(INPUT_DIR_ABS) else str(BASE_DIR / "inputs" / "c_financial_reports_test")
OUTPUT_DIR = OUTPUT_DIR_ABS if os.path.exists(OUTPUT_DIR_ABS) else str(BASE_DIR / "outputs" / "a1_ocr_mixed_text_table_GPT")
YAML_RULE_PATH = YAML_RULE_PATH_ABS if os.path.exists(YAML_RULE_PATH_ABS) else str(BASE_DIR / "configs" / "a1_text_only_rules.yaml")

# ========== Ti·ªán √≠ch chung ==========
def sha1_of_text(text: str) -> str:
    return hashlib.sha1((text or "").encode("utf-8")).hexdigest()

def detect_language_safe(text: str) -> str:
    try:
        return detect(text)
    except Exception:
        return "unknown"

def load_yaml_rules(path: str) -> dict:
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    return {}

def match_yaml_meta(file_name: str, rules: dict) -> dict:
    """T√¨m meta m·∫∑c ƒë·ªãnh d·ª±a theo regex trong YAML."""
    if not rules or "files" not in rules:
        return {}
    for rule in rules["files"]:
        if re.search(rule.get("match", ""), file_name, flags=re.I):
            meta = rule.copy()
            meta.pop("match", None)
            return {**rules.get("defaults", {}), **meta}
    return rules.get("defaults", {})

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

# ========== Helpers cho PDF scan ==========
def _rasterize_pdf_pages(pdf_path: Path, zoom: float = 2.0) -> List[Image.Image]:
    """Tr·∫£ v·ªÅ list PIL.Image t·ª´ m·ªói trang PDF (scan ho·∫∑c vector)."""
    imgs: List[Image.Image] = []
    with fitz.open(str(pdf_path)) as doc:
        mat = fitz.Matrix(zoom, zoom)  # ~144dpi n·∫øu zoom=2.0
        for page in doc:
            pix = page.get_pixmap(matrix=mat, alpha=False)
            mode = "RGB" if pix.n < 5 else "CMYK"
            img = Image.frombytes(mode, (pix.width, pix.height), pix.samples)
            imgs.append(img)
    return imgs

def _ocr_pil(pil_img: Image.Image, lang: str = OCR_LANG) -> str:
    """OCR tr·ª±c ti·∫øp PIL.Image b·∫±ng Tesseract (kh√¥ng c·∫ßn bytes)."""
    return pytesseract.image_to_string(pil_img, lang=lang, config=OCR_CFG) or ""

def _save_png_debug(img: Image.Image, out_dir: Path, name: str):
    if not SAVE_PNG_DEBUG:
        return
    out = out_dir / f"{name}.png"
    try:
        img.save(out)
    except Exception:
        pass

# ========== X·ª≠ l√Ω DOCX ==========
def read_docx_text_and_tables(file_path: Path) -> List[Dict[str, str]]:
    """ƒê·ªçc n·ªôi dung t·ª´ DOCX, t√°ch gi·ªØa ƒëo·∫°n vƒÉn v√† b·∫£ng."""
    doc = Document(str(file_path))
    results = []

    for block in doc.element.body:
        if block.tag.endswith("tbl"):  # B·∫£ng
            rows = []
            for r in block.findall(".//w:tr", block.nsmap):
                cells = [c.text for c in r.findall(".//w:t", block.nsmap)]
                rows.append(" | ".join(cells))
            text_block = "\n".join(rows)
            results.append({"type": "table", "content": text_block})
        else:  # ƒêo·∫°n vƒÉn
            text = "".join(t.text for t in block.findall(".//w:t", block.nsmap)).strip()
            if text:
                results.append({"type": "paragraph", "content": text})
    return results

# ========== X·ª≠ l√Ω PDF (gi·ªØ nguy√™n + th√™m fallback scan) ==========
def read_pdf_text_and_images(file_path: Path, out_dir: Optional[Path] = None,
                             page_start: Optional[int] = None,
                             page_end: Optional[int] = None) -> List[Dict[str, str]]:
    """
    ƒê·ªçc PDF: text + ·∫£nh (OCR ·∫£nh khi c·∫ßn).
    Fallback: n·∫øu trang kh√¥ng c√≥ text/·∫£nh ‚Üí rasterize to√†n trang + OCR (PDF scan).
    Cho ph√©p gi·ªõi h·∫°n ph·∫°m vi trang [page_start, page_end] (1-based, inclusive).
    """
    results: List[Dict[str, str]] = []

    # M·ªü fitz 1 l·∫ßn (d√πng cho fallback)
    try:
        fitz_doc = fitz.open(str(file_path))
    except Exception as e:
        print(f"‚ö†Ô∏è fitz.open l·ªói {file_path}: {e}")
        fitz_doc = None

    # T√≠nh ph·∫°m vi trang (1-based)
    try:
        total_pages = fitz_doc.page_count if fitz_doc else None
    except Exception:
        total_pages = None
    if page_start is None: page_start = 1
    if page_end   is None and total_pages: page_end = total_pages
    if page_end   is None: page_end = page_start
    # ƒë·∫£m b·∫£o h·ª£p l·ªá
    page_start = max(1, page_start)
    page_end   = max(page_start, page_end)

    try:
        with pdfplumber.open(str(file_path)) as pdf:
            # clamp theo s·ªë trang th·ª±c
            if total_pages:
                page_end = min(page_end, total_pages)

            for pi in range(page_start, page_end + 1):
                page = pdf.pages[pi - 1]
                page_had_content = False

                # 1) Text g·ªëc
                try:
                    text = page.extract_text() or ""
                except Exception:
                    text = ""
                if text.strip():
                    results.append({"type": "paragraph", "content": f"[PAGE {pi}][TEXT]\n{text}"})
                    page_had_content = True

                # 2) OCR v√πng ·∫£nh nh√∫ng
                try:
                    if getattr(page, "images", None):
                        for img in page.images:
                            x0, y0, x1, y1 = img["x0"], img["y0"], img["x1"], img["y1"]
                            pil_img = page.within_bbox((x0, y0, x1, y1)).to_image(resolution=300).original
                            if not isinstance(pil_img, Image.Image):
                                pil_img = Image.fromarray(pil_img)
                            pil_img = pil_img.convert("RGB")
                            ocr_txt = _ocr_pil(pil_img, lang=OCR_LANG)
                            if ocr_txt.strip():
                                results.append({"type": "table", "content": f"[PAGE {pi}][OCR_IMG]\n{ocr_txt}"})
                                page_had_content = True
                except Exception:
                    pass

                # 3) Fallback to√†n trang n·∫øu r·ªóng
                if not page_had_content and fitz_doc is not None:
                    try:
                        pg = fitz_doc.load_page(pi - 1)
                        pix = pg.get_pixmap(matrix=fitz.Matrix(2.0, 2.0), alpha=False)
                        mode = "RGB" if pix.n < 5 else "CMYK"
                        pil_full = Image.frombytes(mode, (pix.width, pix.height), pix.samples).convert("RGB")
                        ocr_full = _ocr_pil(pil_full, lang=OCR_LANG)
                        if ocr_full.strip():
                            results.append({"type": "paragraph", "content": f"[PAGE {pi}][OCR_PAGE]\n{ocr_full}"})
                    except Exception as e:
                        print(f"‚ö†Ô∏è PDF OCR fallback error (page {pi}): {e}")

    except Exception as e:
        # pdfplumber h·ªèng ‚Üí rasterize theo ph·∫°m vi
        print(f"‚ö†Ô∏è PDF ƒë·ªçc l·ªói {file_path}: {e}")
        if fitz_doc is not None:
            try:
                if total_pages:
                    page_end = min(page_end, total_pages)
                for pi in range(page_start, page_end + 1):
                    pg = fitz_doc.load_page(pi - 1)
                    pix = pg.get_pixmap(matrix=fitz.Matrix(2.0, 2.0), alpha=False)
                    mode = "RGB" if pix.n < 5 else "CMYK"
                    pil_full = Image.frombytes(mode, (pix.width, pix.height), pix.samples).convert("RGB")
                    ocr_txt = _ocr_pil(pil_full, lang=OCR_LANG)
                    if ocr_txt.strip():
                        results.append({"type": "paragraph", "content": f"[PAGE {pi}][OCR_PAGE]\n{ocr_txt}"})
            except Exception as e2:
                print(f"‚ö†Ô∏è PDF rasterize error {file_path}: {e2}")

    try:
        if fitz_doc is not None:
            fitz_doc.close()
    except Exception:
        pass

    return results



# ========== X·ª≠ l√Ω h√¨nh ·∫£nh ==========
def read_docx_text_and_tables(file_path: Path) -> List[Dict[str, str]]:
    """
    ƒê·ªçc DOCX b·∫±ng API python-docx (tr√°nh XML th√¥).
    - ƒêo·∫°n vƒÉn: p.text
    - B·∫£ng: n·ªëi cell theo h√†ng, ' | ' gi·ªØa cell
    """
    doc = Document(str(file_path))
    results: List[Dict[str, str]] = []

    # paragraphs
    for p in doc.paragraphs:
        txt = (p.text or "").strip()
        if txt:
            results.append({"type": "paragraph", "content": txt})

    # tables
    for tbl in doc.tables:
        rows_text = []
        for row in tbl.rows:
            cells = []
            for cell in row.cells:
                cells.append((cell.text or "").replace("\n", " ").strip())
            rows_text.append(" | ".join(cells))
        if rows_text:
            results.append({"type": "table", "content": "\n".join(rows_text)})

    return results

# ========== X·ª≠ l√Ω TXT ==========
def read_txt(file_path: Path) -> str:
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

# ========== X·ª≠ l√Ω Excel (multi-sheet) ==========
def read_excel_as_text(file_path: Path) -> List[Dict[str, str]]:
    """
    ƒê·ªçc file Excel (.xlsx, .xls) g·ªìm nhi·ªÅu sheet.
    M·ªói sheet tr·∫£ v·ªÅ 1 block {"sheet": <t√™n>, "content": <text b·∫£ng>}.
    """
    results = []
    try:
        sheets = pd.read_excel(str(file_path), sheet_name=None, header=None)
        for sheet_name, df in sheets.items():
            df = df.fillna("")
            rows = []
            for row in df.values.tolist():
                row_text = " | ".join(str(cell).strip() for cell in row)
                rows.append(row_text)
            table_text = "\n".join(rows)
            results.append({"sheet": sheet_name, "content": table_text})
    except Exception as e:
        print(f"‚ö†Ô∏è Excel ƒë·ªçc l·ªói {file_path}: {e}")
    return results


def _post_clean(text: str) -> str:
    """
    Clean nh·∫π:
    - G·ªôp ch·ªØ s·ªë b·ªã t√°ch b·ªüi kho·∫£ng tr·∫Øng (12 345 -> 12345)
    - Xo√° '|' l·∫ª ·ªü cu·ªëi d√≤ng
    - Chu·∫©n ho√° kho·∫£ng tr·∫Øng th·ª´a
    """
    if not text:
        return text
    text = re.sub(r'(?<=\d)\s+(?=\d)', '', text)              # g·ªôp s·ªë b·ªã ch·∫ª
    text = re.sub(r'\|\s*$', '', text, flags=re.MULTILINE)    # x√≥a '|' cu·ªëi d√≤ng
    text = re.sub(r'[ \t]{2,}', ' ', text)                    # thu g·ªçn nhi·ªÅu space/tab
    return text


def ocr_image_to_text(file_path: Path) -> str:
    """OCR ·∫£nh sang text (·ªïn ƒë·ªãnh) ‚Äî lu√¥n √©p RGB tr∆∞·ªõc khi ƒë∆∞a v√†o Tesseract."""
    img = Image.open(str(file_path)).convert("RGB")
    return pytesseract.image_to_string(img, lang=OCR_LANG, config=OCR_CFG)


# ========== X·ª≠ l√Ω ch√≠nh ==========
def process_file(file_path: Path, yaml_rules: dict):
    file_name = file_path.stem
    output_dir = Path(OUTPUT_DIR)
    ensure_dir(output_dir)

    meta = {
        "file": file_name,
        "source_path": str(file_path.resolve()),
        "ocr_lang": OCR_LANG,
        "ocr_cfg": OCR_CFG,
    }
    meta.update(match_yaml_meta(file_name, yaml_rules))

    ext = file_path.suffix.lower()
    has_table = False
    gpt_applied = False
    combined_text = ""

    try:
        if ext == ".docx":
            blocks = read_docx_text_and_tables(file_path)
            for block in blocks:
                if block["type"] == "table":
                    has_table = True
                    gpt_applied = True
                    new_txt = _enhance_with_gpt(block["content"], meta, None)
                    combined_text += "\n" + new_txt
                else:
                    combined_text += "\n" + block["content"]

        elif ext == ".pdf":
            # N·∫øu PAGE_MODE=1 ‚Üí hi·ªÉu start/end l√† ph·∫°m vi trang
            PAGE_MODE = os.getenv("PAGE_MODE", "0") == "1"
            page_start = None
            page_end = None
            if PAGE_MODE and "_page_start" in meta and "_page_end" in meta:
                page_start = meta["_page_start"]
                page_end   = meta["_page_end"]

            blocks = read_pdf_text_and_images(file_path, out_dir=output_dir,
                                              page_start=page_start, page_end=page_end)
            for block in blocks:
                if block["type"] == "table":
                    has_table = True
                    gpt_applied = True
                    new_txt = _enhance_with_gpt(block["content"], meta, None)
                    combined_text += "\n" + new_txt
                else:
                    combined_text += "\n" + block["content"]


        elif ext in [".jpg", ".jpeg", ".png", ".tif", ".tiff", ".bmp", ".webp"]:
            has_table = True
            gpt_applied = True
            txt = ocr_image_to_text(file_path)
            combined_text += "\n" + _enhance_with_gpt(txt, meta, str(file_path))

        elif ext in [".txt", ".csv"]:
            txt = read_txt(file_path)
            combined_text += "\n" + txt

        elif ext in [".xlsx", ".xls"]:
            excel_blocks = read_excel_as_text(file_path)
            if excel_blocks:
                has_table = True
                gpt_applied = True
                all_text = "\n\n".join([f"--- Sheet: {b['sheet']} ---\n{b['content']}" for b in excel_blocks])
                enhanced_txt = _enhance_with_gpt(all_text, meta, None)
                combined_text += "\n" + enhanced_txt

        else:
            print(f"‚ö†Ô∏è Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng {ext}")
            return



        # Clean nh·∫π ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng truy v·∫•n s·ªë/b·∫£ng
        combined_text = _post_clean(combined_text)

        # N·∫øu v·∫´n r·ªóng ‚Üí th·ª≠ fallback l·∫ßn cu·ªëi cho PDF/·∫¢nh (zoom cao h∆°n)
        if not combined_text.strip():
            if ext == ".pdf":
                print(f"‚ö†Ô∏è {file_name}: combined_text tr·ªëng ‚Üí fallback rasterize zoom=3.0")
                try:
                    with fitz.open(str(file_path)) as _doc:
                        chunks = []
                        for pi in range(_doc.page_count):
                            pg = _doc.load_page(pi)
                            pix = pg.get_pixmap(matrix=fitz.Matrix(3.0, 3.0), alpha=False)
                            mode = "RGB" if pix.n < 5 else "CMYK"
                            pil_full = Image.frombytes(mode, (pix.width, pix.height), pix.samples).convert("RGB")
                            ocr_full = _ocr_pil(pil_full, lang=OCR_LANG)
                            if ocr_full.strip():
                                chunks.append(f"[PAGE {pi+1}][OCR_PAGE]\n{ocr_full}")
                        combined_text = "\n\n".join(chunks)
                except Exception as _e:
                    print(f"‚ö†Ô∏è Fallback PDF zoom=3.0 l·ªói: {_e}")

            elif ext in [".jpg", ".jpeg", ".png", ".tif", ".tiff", ".bmp", ".webp"]:
                print(f"‚ö†Ô∏è {file_name}: OCR ·∫£nh r·ªóng ‚Üí th·ª≠ l·∫°i convert RGB & psm=6")
                try:
                    combined_text = ocr_image_to_text(file_path)
                except Exception as _e:
                    print(f"‚ö†Ô∏è Fallback OCR ·∫£nh l·ªói: {_e}")

        # enrich meta
        lang = detect_language_safe(combined_text)
        meta.update({
            "language": lang,
            "has_table": has_table,
            "gpt_applied": gpt_applied,
            "text_sha1": sha1_of_text(combined_text),
            "empty_after_fallback": not bool(combined_text.strip())
        })

        # N·∫øu v·∫´n tr·ªëng ho√†n to√†n ‚Üí ghi marker ƒë·ªÉ tr√°nh file 0 KB
        if not combined_text.strip():
            combined_text = "[EMPTY] No text extracted by OCR/PDF. See meta.empty_after_fallback=true"

        # Ghi file output
        txt_out = output_dir / f"{file_name}_text.txt"
        meta_out = output_dir / f"{file_name}_meta.json"
        with open(txt_out, "w", encoding="utf-8") as f:
            f.write(combined_text.strip())
        with open(meta_out, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ Saved: {txt_out.name}, {meta_out.name}  | len={len(combined_text)}")

    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω {file_path}: {e}")

# ========== CLI ==========
def main():
    parser = argparse.ArgumentParser(
        description="OCR + GPT cho t√†i li·ªáu h·ªón h·ª£p (text + b·∫£ng + ·∫£nh)"
    )
    parser.add_argument("--clean", choices=["y", "n"], default="n")
    parser.add_argument("--start", type=int, default=1, help="S·ªë th·ª© t·ª± file b·∫Øt ƒë·∫ßu (t√≠nh t·ª´ 1) ho·∫∑c TRANG b·∫Øt ƒë·∫ßu n·∫øu PAGE_MODE=1")
    parser.add_argument("--end", type=int, default=None, help="S·ªë th·ª© t·ª± file k·∫øt th√∫c (ho·∫∑c TRANG k·∫øt th√∫c n·∫øu PAGE_MODE=1)")
    parser.add_argument("--file_index", type=int, default=1, help="(Ch·ªâ khi PAGE_MODE=1) ch·ªçn file th·ª© m·∫•y trong INPUT ƒë·ªÉ c·∫Øt trang (m·∫∑c ƒë·ªãnh 1)")
    args = parser.parse_args()

    yaml_rules = load_yaml_rules(YAML_RULE_PATH)
    files = sorted(list(Path(INPUT_DIR).rglob("*.*")))

    PAGE_MODE = os.getenv("PAGE_MODE", "0") == "1"

    if PAGE_MODE:
        # Hi·ªÉu --start/--end l√† trang
        # Ch·ªçn file PDF theo --file_index (1-based)
        pdfs = [f for f in files if f.suffix.lower() == ".pdf"]
        if not pdfs:
            print("‚ö†Ô∏è PAGE_MODE=1 nh∆∞ng kh√¥ng t√¨m th·∫•y PDF n√†o trong INPUT_DIR.")
            return
        file_idx = max(1, min(args.file_index, len(pdfs))) - 1
        target = pdfs[file_idx]

        # L∆∞u th√¥ng tin trang v√†o meta th√¥ng qua bi·∫øn t·∫°m
        global _PAGE_RANGE_CACHE
        _PAGE_RANGE_CACHE = (args.start, args.end)

        print("=== C·∫§U H√åNH HI·ªÜN T·∫†I (PAGE_MODE=1) ===")
        print(f"üìÇ INPUT_DIR   : {INPUT_DIR}")
        print(f"üì¶ OUTPUT_DIR  : {OUTPUT_DIR}")
        print(f"‚öôÔ∏è YAML_RULE   : {YAML_RULE_PATH}")
        print(f"üéØ FILE        : {target.name}")
        print(f"üìÑ PAGES       : {args.start} ‚Üí {args.end}")
        print("==========================")

        # G·ªçi process_file ch·ªâ cho 1 file PDF, v√† b∆°m _page_start/_page_end v√†o meta
        def _process_pdf_with_pages(fpath: Path, rules: dict):
            # b∆°m v√†o meta qua match_yaml_meta ‚Üí c√°ch nhanh l√† set v√†o rules.defaults t·∫°m th·ªùi
            # tr√°nh side-effects: copy rules
            rules = dict(rules) if rules else {}
            defaults = dict(rules.get("defaults", {}))
            defaults["_page_start"] = args.start
            defaults["_page_end"] = args.end if args.end else args.start
            rules["defaults"] = defaults
            process_file(fpath, rules)

        _process_pdf_with_pages(Path(target), yaml_rules)

    else:
        # H√†nh vi c≈©: --start/--end l√† ch·ªâ s·ªë file
        selected = files[args.start - 1 : args.end] if args.end else files[args.start - 1 :]

        print("=== C·∫§U H√åNH HI·ªÜN T·∫†I ===")
        print(f"üìÇ INPUT_DIR   : {INPUT_DIR}")
        print(f"üì¶ OUTPUT_DIR  : {OUTPUT_DIR}")
        print(f"‚öôÔ∏è YAML_RULE   : {YAML_RULE_PATH}")
        print(f"üß† T·ªïng s·ªë file: {len(files)} | S·∫Ω x·ª≠ l√Ω: {len(selected)} (t·ª´ {args.start} ƒë·∫øn {args.end or len(files)})")
        print("==========================")

        for file_path in tqdm(selected, desc="Processing"):
            process_file(Path(file_path), yaml_rules)

    print("üéØ Ho√†n t·∫•t A2. Ki·ªÉm tra th∆∞ m·ª•c output ƒë·ªÉ xem k·∫øt qu·∫£.")


if __name__ == "__main__":
    main()
