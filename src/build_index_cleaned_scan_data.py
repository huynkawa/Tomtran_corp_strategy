# === D√πng ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu ƒë√£ qua OCR & ƒë√£ clean (Excel, CSV, TXT) t·ª´ scan ===

import os
import shutil
import json
from pathlib import Path
from typing import List

import pandas as pd
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_chroma import Chroma
from dotenv import load_dotenv
from src.config import make_embeddings
from tqdm import tqdm   # ‚úÖ hi·ªÉn th·ªã progress bar
from termcolor import colored  # ‚úÖ ƒë·ªÉ in highlight

# === C·∫•u h√¨nh ===
load_dotenv()
FINAL_DIR = "inputs/cleaned_scan_input"  # ƒë·∫ßu ra c·ªßa clean_ocr.py
VECTOR_DIR = os.getenv("VECTOR_DIR", "vector_store")

HIGHLIGHT_KEYWORDS = ["doanh thu", "ph√≠ b·∫£o hi·ªÉm", "b·∫£o hi·ªÉm", "doanhthu"]

# === T·∫°o m√¥ t·∫£ b·∫£ng t·ª´ DataFrame ===
def table_to_text(df: pd.DataFrame, source: str, table_name: str = "") -> str:
    lines = []
    columns = [str(c).strip() for c in df.columns]
    if table_name:
        lines.append(f"B·∫£ng: {table_name}")
    lines.append("Ti√™u ƒë·ªÅ c·ªôt: " + " | ".join(columns))
    for idx, row in df.iterrows():
        row_str = ", ".join(f"{columns[i]} = {str(val).strip()}" for i, val in enumerate(row))
        lines.append(f"D√≤ng {idx+1}: {row_str}")
    return "\n".join(lines)

# === In preview c√≥ highlight ===
def preview_with_highlight(df: pd.DataFrame, file: str, sheet: str = None):
    try:
        preview_df = df.head(5).copy()
        preview_str = ""
        for col in preview_df.columns:
            col_str = str(col)
            if any(k in col_str.lower() for k in HIGHLIGHT_KEYWORDS):
                col_display = colored(col_str, "red", attrs=["bold"])
            else:
                col_display = col_str
            preview_str += f"{col_display}\t"
        preview_str += "\n"

        for _, row in preview_df.iterrows():
            row_display = []
            for col, val in row.items():
                val_str = str(val)
                if any(k in val_str.lower() for k in HIGHLIGHT_KEYWORDS):
                    row_display.append(colored(val_str, "red", attrs=["bold"]))
                else:
                    row_display.append(val_str)
            preview_str += "\t".join(row_display) + "\n"

        print(f"\nüìä Preview b·∫£ng t·ª´ {file} (sheet {sheet}):\n{preview_str}\n{'-'*60}")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng in preview ƒë∆∞·ª£c cho {file}: {e}")

# === Chunk vƒÉn b·∫£n ===
def chunk_documents(docs: List[Document], chunk_size=900, chunk_overlap=120) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    all_chunks = []
    for doc in docs:
        if doc.metadata.get("type") == "table":
            all_chunks.append(doc)
        else:
            all_chunks.extend(splitter.split_documents([doc]))
    return all_chunks

# === Load file clean t·ª´ OCR (Excel, CSV, TXT + metadata) ===
def load_clean_docs(dir_path: str) -> List[Document]:
    docs = []
    for file in Path(dir_path).rglob("*"):
        ext = file.suffix.lower()
        try:
            if ext == ".xlsx":
                xl = pd.ExcelFile(file)
                for sheet in xl.sheet_names:
                    df = xl.parse(sheet)
                    preview_with_highlight(df, str(file), sheet)
                    docs.append(Document(
                        page_content=table_to_text(df, str(file), f"Sheet {sheet}"),
                        metadata={"source": str(file), "sheet": sheet, "type": "table"}
                    ))
            elif ext == ".csv":
                df = pd.read_csv(file)
                preview_with_highlight(df, str(file), "CSV")
                docs.append(Document(
                    page_content=table_to_text(df, str(file), f"CSV {file.stem}"),
                    metadata={"source": str(file), "type": "table"}
                ))
            elif ext == ".txt":
                docs_txt = TextLoader(str(file), encoding="utf-8").load()
                base = str(file).replace("_text.txt", "")
                meta_file = base + "_meta.json"
                if os.path.exists(meta_file):
                    with open(meta_file, "r", encoding="utf-8") as f:
                        meta = json.load(f)
                    for d in docs_txt:
                        d.metadata.update(meta)
                docs.extend(docs_txt)
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file {file}: {e}")
    return docs

# === Main ===
if __name__ == "__main__":
    final_root = Path(FINAL_DIR)

    for subdir in final_root.iterdir():
        if subdir.is_dir():
            sub_vector_dir = os.path.join(VECTOR_DIR, "cleaned_scan_data", subdir.name)

            # ‚úÖ N·∫øu ƒë√£ t·ªìn t·∫°i vector cho subdir n√†y ‚Üí h·ªèi Yes/No
            if os.path.exists(sub_vector_dir):
                choice = input(
                    f"‚ö†Ô∏è Vector store {sub_vector_dir} ƒë√£ t·ªìn t·∫°i. "
                    f"B·∫°n c√≥ mu·ªën xo√° v√† build l·∫°i t·ª´ ƒë·∫ßu? (y/n): "
                ).strip().lower()
                if choice == "y":
                    shutil.rmtree(sub_vector_dir)
                    print(f"üóëÔ∏è ƒê√£ xo√° vector c≈©: {sub_vector_dir}")
                else:
                    print(f"‚è≠Ô∏è B·ªè qua {subdir.name}, gi·ªØ d·ªØ li·ªáu c≈©.")
                    continue

            print(f"\nüì• ƒêang n·∫°p d·ªØ li·ªáu t·ª´: {subdir}")

            docs = load_clean_docs(str(subdir))
            chunks = chunk_documents(docs)
            print(f"[ingest] T·ªïng s·ªë chunks: {len(chunks)}")

            os.makedirs(sub_vector_dir, exist_ok=True)

            embeddings = make_embeddings()
            vectordb = Chroma.from_documents(
                documents=tqdm(chunks, desc=f"[embedding] {subdir.name}", unit="chunk"),
                embedding=embeddings,
                persist_directory=sub_vector_dir
            )

            print("\n=== T√ìM T·∫ÆT ===")
            print(f"üìÇ Th∆∞ m·ª•c: {subdir.name}")
            print(f"üìÑ T·ªïng document: {len(docs)}")
            print(f"üß© T·ªïng chunks: {len(chunks)}")
            print(f"üìä B·∫£ng (gi·ªØ nguy√™n): {len([d for d in chunks if d.metadata.get('type') == 'table'])}")
            print(f"‚úÖ Vector l∆∞u t·∫°i: {sub_vector_dir}")
