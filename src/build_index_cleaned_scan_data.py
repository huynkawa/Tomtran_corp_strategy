# ğŸ“ src/build_index_scan_data.py
# === DÃ¹ng Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u Ä‘Ã£ qua OCR & Ä‘Ã£ clean (Excel, CSV, TXT) tá»« scan ===

import os
import shutil
from pathlib import Path
from typing import List

import pandas as pd
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader, DirectoryLoader, Docx2txtLoader, TextLoader
)
from langchain_chroma import Chroma
from dotenv import load_dotenv
from src.config import make_embeddings

# === Cáº¥u hÃ¬nh ===
load_dotenv()
FINAL_DIR = "inputs/cleaned_scan_input"  # Ä‘áº§u ra cá»§a clean_ocr.py
VECTOR_DIR = os.getenv("VECTOR_DIR", "vector_store")

# === Táº¡o mÃ´ táº£ báº£ng tá»« DataFrame ===
def table_to_text(df: pd.DataFrame, source: str, table_name: str = "") -> str:
    lines = []
    columns = [str(c).strip() for c in df.columns]
    if table_name:
        lines.append(f"Báº£ng: {table_name}")
    lines.append("TiÃªu Ä‘á» cá»™t: " + " | ".join(columns))
    for idx, row in df.iterrows():
        row_str = ", ".join(f"{columns[i]} = {str(val).strip()}" for i, val in enumerate(row))
        lines.append(f"DÃ²ng {idx+1}: {row_str}")
    return "\n".join(lines)

# === Chunk vÄƒn báº£n ===
def chunk_documents(docs: List[Document], chunk_size=900, chunk_overlap=120) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    all_chunks = []
    for doc in docs:
        if doc.metadata.get("type") == "table":
            all_chunks.append(doc)
        else:
            all_chunks.extend(splitter.split_documents([doc]))
    return all_chunks

# === Load file clean tá»« OCR (Excel, CSV, TXT) ===
def load_clean_docs(dir_path: str) -> List[Document]:
    docs = []
    for file in Path(dir_path).rglob("*"):
        ext = file.suffix.lower()
        try:
            if ext == ".xlsx":
                xl = pd.ExcelFile(file)
                for sheet in xl.sheet_names:
                    df = xl.parse(sheet)
                    docs.append(Document(
                        page_content=table_to_text(df, str(file), f"Sheet {sheet}"),
                        metadata={"source": str(file), "sheet": sheet, "type": "table"}
                    ))
            elif ext == ".csv":
                df = pd.read_csv(file)
                docs.append(Document(
                    page_content=table_to_text(df, str(file), f"CSV {file.stem}"),
                    metadata={"source": str(file), "type": "table"}
                ))
            elif ext == ".txt":
                docs.extend(TextLoader(str(file), encoding="utf-8").load())
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c file {file}: {e}")
    return docs

# === Main ===
from pathlib import Path

if __name__ == "__main__":
    if os.path.exists(VECTOR_DIR):
        shutil.rmtree(VECTOR_DIR)
        print(f"ğŸ—‘ï¸ ÄÃ£ xoÃ¡ vector cÅ©: {VECTOR_DIR}")

    final_root = Path(FINAL_DIR)
    for subdir in final_root.iterdir():
        if subdir.is_dir():
            print(f"\nğŸ“¥ Äang náº¡p dá»¯ liá»‡u tá»«: {subdir}")

            docs = load_clean_docs(str(subdir))   # hoáº·c load_documents náº¿u file báº¡n cÃ³
            chunks = chunk_documents(docs)

            sub_vector_dir = os.path.join(VECTOR_DIR, subdir.name)
            os.makedirs(sub_vector_dir, exist_ok=True)

            vectordb = Chroma.from_documents(
                documents=chunks,
                embedding=make_embeddings(),
                persist_directory=sub_vector_dir
            )

            print("\n=== TÃ“M Táº®T ===")
            print(f"ğŸ“‚ ThÆ° má»¥c: {subdir.name}")
            print(f"ğŸ“„ Tá»•ng document: {len(docs)}")
            print(f"ğŸ§© Tá»•ng chunks: {len(chunks)}")
            print(f"ğŸ“Š Báº£ng (giá»¯ nguyÃªn): {len([d for d in chunks if d.metadata.get('type') == 'table'])}")
            print(f"âœ… Vector lÆ°u táº¡i: {sub_vector_dir}")
