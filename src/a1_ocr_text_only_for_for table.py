# -*- coding: utf-8 -*-
"""
clean10_ocr_pipeline.py ‚Äî OCR th√¥ (PPStructure + Tesseract) cho PDF scan.

Outputs (mirror th∆∞ m·ª•c input):
  outputs/orc_raw_output/<mirror>/<base>_page{n}.xlsx   (n·∫øu PPStructure ra b·∫£ng t·ªët)
  outputs/orc_raw_output/<mirror>/<base>_page{n}.csv    (fallback Tesseract: schema 5 c·ªôt)
  outputs/orc_raw_output/<mirror>/<base>_page{n}_excel.txt
  outputs/orc_raw_output/<mirror>/<base>_page{n}_text.txt
  outputs/orc_raw_output/<mirror>/<base>_page{n}_meta.json
"""

import os, re, cv2, glob, json, shutil, argparse
import numpy as np
import pandas as pd
from typing import Optional, List, Tuple
from pdf2image import convert_from_path
from paddleocr import PPStructure, save_structure_res
import pytesseract
from sklearn.cluster import KMeans

# ====== C·∫•u h√¨nh ======
INPUT_FILE_DEFAULT = None
INPUT_DIR_DEFAULT  = r"inputs/a_text_only_inputs"
OUTPUT_DIR_DEFAULT = r"outputs/a_text_only_outputs"
DPI_DEFAULT        = 500           # DPI cao h∆°n ƒë·ªÉ n√©t h∆°n
MIN_ROWS_DEFAULT   = 5             # Excel < ng∆∞·ª°ng coi nh∆∞ fail v√† fallback
POPPLER_PATH       = os.environ.get("POPPLER_PATH", None)
TESSERACT_CMD      = os.environ.get("TESSERACT_CMD", None)
if TESSERACT_CMD and os.path.isfile(TESSERACT_CMD):
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD

# ====== Utils ======
def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def detect_unit(text: str) -> Optional[str]:
    for p in (r"ƒê∆°n v·ªã\s*[:\-]\s*(.+)", r"Don vi\s*[:\-]\s*(.+)", r"Unit\s*[:\-]\s*(.+)"):
        m = re.search(p, text, flags=re.IGNORECASE)
        if m:
            return m.group(1).strip().splitlines()[0]
    return None

def excel_to_text(excel_path: str, source: str) -> str:
    try:
        df = pd.read_excel(excel_path, header=None, engine="openpyxl")
        lines = []
        for row in df.itertuples(index=False):
            cells = [str(c).strip() for c in row if pd.notna(c)]
            row_text = " | ".join([c for c in cells if c])
            if row_text:
                lines.append(row_text)
        table_text = "\n".join(lines)
        return f"[TABLE from {source}]\n{table_text}"
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c Excel {excel_path}: {e}")
        return ""

# ====== Chu·∫©n ho√° b·∫£ng ======
EXPECTED_COLS = ["M√£ s·ªë", "Kho·∫£n m·ª•c/T√†i s·∫£n", "Thuy·∫øt minh", "S·ªë cu·ªëi nƒÉm", "S·ªë ƒë·∫ßu nƒÉm"]

def _is_number_like(s: str) -> bool:
    return bool(re.match(r"^[+-]?\d[\d\.,]*$", str(s).strip()))

SECTION_RE = re.compile(r"^(?:[A-Z]\.?|[IVXLC]+\.)$")   # A. | B. | I. | II. | III.
NOTE_SHORT_NUM = re.compile(r"^\d{1,3}$")               # 1..3 -> c√≥ th·ªÉ l√† s·ªë thuy·∫øt minh

def _is_number_like(s: str) -> bool:
    return bool(re.match(r"^[+-]?\d[\d\.,]*$", str(s).strip()))

CODE_RE = re.compile(r"^\d{3}(?:\.\d+)?$")  # 100, 131.1, 151.2
NOTE_SHORT_NUM = re.compile(r"^\d{1,3}$")
SECTION_RE = re.compile(r"^(?:[A-Z]\.?|[IVXLC]+\.)$")   # A., B., I., II., ...

def _clean_token(s: str) -> str:
    if s is None:
        return ""
    t = str(s)
    # b·ªè k√Ω t·ª± k·∫ª b·∫£ng r∆°i v√†o text
    t = t.replace("|", " ").replace("¬¶", " ").replace("¬∑", " ")
    # r√∫t g·ªçn kho·∫£ng tr·∫Øng
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _is_number_like(s: str) -> bool:
    return bool(re.match(r"^[+-]?\d[\d\.,]*$", str(s).strip()))

def auto_postprocess_table(df: pd.DataFrame) -> pd.DataFrame:
    """
    Chu·∫©n ho√° b·∫£ng v·ªÅ schema 5 c·ªôt:
    - L√†m s·∫°ch token (b·ªè '|')
    - N·∫øu 'M√£ s·ªë' kh√¥ng kh·ªõp CODE_RE -> ƒë·∫©y sang 'Kho·∫£n m·ª•c/T√†i s·∫£n'
    - Gh√©p d√≤ng wrap: (code r·ªóng/kh√¥ng h·ª£p l·ªá) & kh√¥ng c√≥ s·ªë li·ªáu
    - ƒê·∫©y 'item' to√†n s·ªë sang 'S·ªë cu·ªëi nƒÉm' (tr·ª´ s·ªë thuy·∫øt minh ng·∫Øn)
    - Chu·∫©n ho√° s·ªë v·ªÅ numeric
    """
    EXPECTED_COLS = ["M√£ s·ªë","Kho·∫£n m·ª•c/T√†i s·∫£n","Thuy·∫øt minh","S·ªë cu·ªëi nƒÉm","S·ªë ƒë·∫ßu nƒÉm"]
    if df is None or df.empty:
        return pd.DataFrame(columns=EXPECTED_COLS)

    # √©p ƒë√∫ng 5 c·ªôt
    df = df.reset_index(drop=True).copy()
    if df.shape[1] < 5:
        for _ in range(df.shape[1], 5):
            df[df.shape[1]] = ""
    elif df.shape[1] > 5:
        df = df.iloc[:, :5]
    df.columns = EXPECTED_COLS

    # l√†m s·∫°ch token
    for c in df.columns:
        df[c] = df[c].map(_clean_token)

    rows = []
    for _, r in df.iterrows():
        code, item, note, endv, startv = r.tolist()

        # 'IL' -> 'II'
        if note.upper() == "IL":
            note = "II"

        # n·∫øu 'M√£ s·ªë' KH√îNG h·ª£p l·ªá -> ƒë·∫©y sang 'Kho·∫£n m·ª•c/T√†i s·∫£n'
        if code and not CODE_RE.fullmatch(code):
            item = (code + " " + item).strip()
            code = ""

        # d√≤ng wrap: kh√¥ng m√£ s·ªë (ho·∫∑c m√£ kh√¥ng h·ª£p l·ªá), kh√¥ng c√≥ s·ªë li·ªáu
        if (not code) and (not endv) and (not startv) and rows:
            rows[-1][1] = (rows[-1][1] + " " + item).strip()
            if note:
                rows[-1][2] = (rows[-1][2] + " " + note).strip()
            continue

        # 'item' to√†n s·ªë -> c√≥ th·ªÉ l√† s·ªë li·ªáu
        if _is_number_like(item) and not NOTE_SHORT_NUM.fullmatch(item) and not endv and not startv:
            endv, item = item, ""

        rows.append([code, item, note, endv, startv])

    out = pd.DataFrame(rows, columns=EXPECTED_COLS)

    # chu·∫©n ho√° s·ªë li·ªáu
    for col in ["S·ªë cu·ªëi nƒÉm","S·ªë ƒë·∫ßu nƒÉm"]:
        out[col] = (
            out[col].astype(str)
            .str.replace(r"[^\d\-,]", "", regex=True)
            .replace("", None)
        )
        out[col] = pd.to_numeric(out[col].str.replace(",", ""), errors="coerce")

    return out



def check_subtotals(df: pd.DataFrame, tol: int = 5) -> pd.DataFrame:
    """ƒê√°nh d·∫•u l·ªách subtotal (n·∫øu c·∫ßn)."""
    if df.empty:
        return df
    df = df.copy()
    df["Ki·ªÉm tra"] = ""
    # (gi·ªØ stub, tu·ª≥ d·ª± √°n c√≥ th·ªÉ b·ªï sung c√¥ng th·ª©c sum theo c√°c m·ª•c con)
    return df

# --------- ·∫¢nh: deskew nh·∫π + ti·ªÅn x·ª≠ l√Ω borderless ----------
def _deskew_binary(gray: np.ndarray) -> np.ndarray:
    bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]
    bw_inv = 255 - bw
    coords = np.column_stack(np.where(bw_inv > 0))
    if coords.size == 0:
        return gray
    rect = cv2.minAreaRect(coords)
    angle = rect[-1]
    angle = -(90 + angle) if angle < -45 else -angle
    if abs(angle) < 0.2:
        return gray
    (h, w) = gray.shape[:2]
    M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
    return cv2.warpAffine(gray, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)

def preprocess_for_borderless(img_bgr: np.ndarray) -> np.ndarray:
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    gray = _deskew_binary(gray)
    gray = cv2.bilateralFilter(gray, 9, 75, 75)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enh = clahe.apply(gray)
    thr = cv2.threshold(enh, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
    return cv2.cvtColor(thr, cv2.COLOR_GRAY2BGR)

def detect_table_roi(img_bgr: np.ndarray) -> np.ndarray:
    """
    T√¨m ROI b·∫£ng l·ªõn nh·∫•t b·∫±ng morphology ƒë∆∞·ªùng k·∫ª.
    N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c, tr·∫£ v·ªÅ ·∫£nh g·ªëc.
    """
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    thr  = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]
    h, w = thr.shape

    vert = cv2.erode(thr, cv2.getStructuringElement(cv2.MORPH_RECT, (1, max(10, h//60))), 1)
    vert = cv2.dilate(vert, cv2.getStructuringElement(cv2.MORPH_RECT, (1, max(10, h//60))), 2)

    hori = cv2.erode(thr, cv2.getStructuringElement(cv2.MORPH_RECT, (max(10, w//60), 1)), 1)
    hori = cv2.dilate(hori, cv2.getStructuringElement(cv2.MORPH_RECT, (max(10, w//60), 1)), 2)

    grid = cv2.bitwise_or(vert, hori)
    contours, _ = cv2.findContours(grid, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return img_bgr

    cnt = max(contours, key=cv2.contourArea)
    x, y, w, h = cv2.boundingRect(cnt)

    pad = 6
    y0 = max(0, y - pad); x0 = max(0, x - pad)
    y1 = min(img_bgr.shape[0], y + h + pad); x1 = min(img_bgr.shape[1], x + w + pad)
    return img_bgr[y0:y1, x0:x1]

def read_pdf_pages(pdf_path: str, start_page: int, end_page: Optional[int], dpi: int):
    kwargs = dict(dpi=dpi, first_page=start_page, last_page=end_page)
    if POPPLER_PATH: kwargs["poppler_path"] = POPPLER_PATH
    pages = convert_from_path(pdf_path, **kwargs)
    end_used = end_page if end_page is not None else (start_page + len(pages) - 1)
    return pages, start_page, end_used

def export_excel_from_ppstructure_result(_result, to_path: str) -> bool:
    ok = False
    try:
        tables = []
        for el in _result or []:
            if isinstance(el, dict) and el.get("type") == "table":
                res = el.get("res") or {}
                html = res.get("html")
                if html:
                    try:
                        dfs = pd.read_html(html)
                        if dfs: tables.append(dfs[0])
                    except Exception as ex:
                        print(f"‚ö†Ô∏è read_html fail: {ex}")
        if tables:
            out_df = pd.concat(tables, ignore_index=True)
            out_df = auto_postprocess_table(out_df)
            with pd.ExcelWriter(to_path, engine="xlsxwriter") as writer:
                out_df.to_excel(writer, index=False, sheet_name="Sheet1")
                wb = writer.book; ws = writer.sheets["Sheet1"]
                num_fmt = wb.add_format({"num_format": "#,##0"})
                ws.set_column("A:A", 9)
                ws.set_column("B:B", 60)
                ws.set_column("C:C", 12)
                ws.set_column("D:E", 22, num_fmt)
            ok = True
    except Exception as ex:
        print(f"‚ö†Ô∏è export_excel_from_ppstructure_result fail: {ex}")
    return ok

# --- helper regex d√πng cho fallback ph√¢n c·ªôt ---
CODE_RE = re.compile(r"^\d{3}(?:\.\d+)?$")           # 100 | 131.1 | 151.2
NOTE_RE = re.compile(r"^(?:[IVXLC]+|\d(?:\.\d+)*)$") # I | II | 1 | 1.1 | 1.2.3
NUM_RE  = re.compile(r"^[+-]?\d[\d.,]*$")            # 3.677.178.873.830 | 1,144,179,317,060

def _is_code_token(s: str) -> bool:
    return bool(CODE_RE.fullmatch(str(s).strip()))

def _is_note_token(s: str) -> bool:
    t = str(s).strip().upper()
    # ch·∫•p nh·∫≠n I, II, III, 1, 1.1, 1.2.3 nh∆∞ng lo·∫°i tr·ª´ c√°c chu·ªói c√≥ 3+ ch·ªØ s·ªë li√™n ti·∫øp (d·ªÖ l√† ti·ªÅn)
    if re.fullmatch(r"(?:[IVXLC]+|\d(?:\.\d+){0,2})", t):
        # n·∫øu l√† s·ªë thu·∫ßn ng·∫Øn 1..3 ch·ªØ s·ªë th√¨ ch·ªâ coi l√† note n·∫øu n√≥ ƒë·ª©ng g·∫ßn c·ªôt "Thuy·∫øt minh"
        return True
    return False


def _is_numeric_token(s: str) -> bool:
    return bool(NUM_RE.fullmatch(str(s).strip()))

def _find_header_anchors(df_words: pd.DataFrame) -> dict | None:
    """
    T√¨m v·ªã tr√≠ x (left) c·ªßa c√°c c·ªôt t·ª´ d√≤ng header: 'M√£ s·ªë', 'Thuy·∫øt minh', 'S·ªë cu·ªëi nƒÉm', 'S·ªë ƒë·∫ßu nƒÉm'.
    Tr·∫£ v·ªÅ dict: {'code': x_code_right_bound, 'note': x_note, 'end': x_end, 'start': x_start}
    N·∫øu kh√¥ng th·∫•y ƒë·ªß, tr·∫£ v·ªÅ None.
    """
    if df_words.empty:
        return None

    def norm(s: str) -> str:
        t = str(s).lower().strip()
        rep = {
            "√¢":"a","ƒÉ":"a","√°":"a","√†":"a","·∫£":"a","√£":"a","·∫°":"a",
            "√™":"e","√©":"e","√®":"e","·∫ª":"e","·∫Ω":"e","·∫π":"e",
            "√¥":"o","∆°":"o","√≥":"o","√≤":"o","·ªè":"o","√µ":"o","·ªç":"o",
            "∆∞":"u","√∫":"u","√π":"u","·ªß":"u","≈©":"u","·ª•":"u",
            "√≠":"i","√¨":"i","·ªâ":"i","ƒ©":"i","·ªã":"i",
            "√Ω":"y","·ª≥":"y","·ª∑":"y","·ªπ":"y","·ªµ":"y","ƒë":"d"
        }
        for k,v in rep.items():
            t = t.replace(k, v)
        return t

    words = df_words.copy()
    words["norm"] = words["text"].map(norm)

    cand_code  = words[words["norm"].str.contains(r"\bma\s*so\b")]
    cand_note  = words[words["norm"].str.contains(r"\bthuyet\s*minh\b")]
    cand_end   = words[words["norm"].str.contains(r"\bso\s*cuoi\b|\bso\s*cuoi\s*nam\b")]
    cand_start = words[words["norm"].str.contains(r"\bso\s*dau\b|\bso\s*dau\s*nam\b")]

    if cand_note.empty or cand_end.empty or cand_start.empty:
        return None

    x_note  = float(cand_note.sort_values("left").iloc[0]["left"])
    x_end   = float(cand_end.sort_values("left").iloc[0]["left"])
    x_start = float(cand_start.sort_values("left").iloc[0]["left"])

    left_min = float(words["left"].min())
    code_right_bound = left_min + (x_note - left_min) * 0.30

    return {"code": code_right_bound, "note": x_note, "end": x_end, "start": x_start}


def _group_lines(df_words: pd.DataFrame) -> list[pd.DataFrame]:
    """Nh√≥m token theo d√≤ng v·ªõi ng∆∞·ª°ng ƒë·ªông t·ª´ median height."""
    lines, cur, last_y = [], [], None
    if df_words.empty: return lines
    thr = max(8.0, float(df_words["height"].median()) * 0.9)
    for _, r in df_words.sort_values(["y_center", "left"]).iterrows():
        y = r["y_center"]
        if last_y is None or abs(y - last_y) <= thr:
            cur.append(r)
        else:
            lines.append(pd.DataFrame(cur)); cur = [r]
        last_y = y
    if cur: lines.append(pd.DataFrame(cur))
    return lines

def tesseract_csv_fallback(page_bgr: np.ndarray, csv_path: str):
    """
    Fallback ch√≠nh x√°c h∆°n:
    - Crop ROI b·∫£ng
    - L·∫•y t√¢m 2 c·ªôt s·ªë t·ª´ token s·ªë; t√¢m 1 c·ªôt note t·ª´ token note
    - Assign c·ªôt d·ª±a tr√™n n·ªôi dung + g·∫ßn t√¢m
    - Gh√©p d√≤ng theo ng∆∞·ª°ng ƒë·ªông
    - B·ªè ph·∫ßn r√°c ƒë·∫øn tr∆∞·ªõc d√≤ng 'm√£ s·ªë' ƒë·∫ßu ti√™n
    - Xu·∫•t CSV + Excel (ƒë·ªãnh d·∫°ng s·ªë, set width)
    """
    try:
        from pytesseract import Output

        roi = detect_table_roi(page_bgr)

        data = pytesseract.image_to_data(roi, lang="eng+vie", output_type=Output.DATAFRAME)
        data = data.dropna(subset=["text"])
        data = data[data["text"].astype(str).str.strip().astype(bool)]
        if data.empty:
            pd.DataFrame(columns=EXPECTED_COLS).to_csv(csv_path, index=False, encoding="utf-8-sig")
            print(f"üìÑ Fallback CSV (tr·ªëng): {csv_path}")
            return

        # to·∫° ƒë·ªô trung t√¢m d·ªçc
        data["y_center"] = data["top"] + data["height"] / 2

        # --- 1) ∆Ø·ªõc l∆∞·ª£ng t√¢m c·ªôt (∆∞u ti√™n neo header n·∫øu c√≥) ---
        anchors = _find_header_anchors(data)

        if anchors:
            center_note   = anchors["note"]
            center_end    = anchors["end"]
            center_start  = anchors["start"]
            code_right_bound = anchors["code"]
        else:
            nums = data[data["text"].apply(_is_numeric_token)]
            if len(nums) >= 4:
                km2 = KMeans(n_clusters=2, n_init=10, random_state=0).fit(nums[["left"]].to_numpy())
                centers_num = sorted(list(km2.cluster_centers_.ravel()))
            else:
                xs = sorted(data["left"].tolist())
                centers_num = [np.percentile(xs, 70), np.percentile(xs, 90)]

            notes = data[data["text"].apply(_is_note_token)]
            if len(notes) >= 3:
                km1 = KMeans(n_clusters=1, n_init=5, random_state=0).fit(notes[["left"]].to_numpy())
                center_note = float(km1.cluster_centers_.ravel()[0])
            else:
                center_note = float(np.percentile(data["left"], 55))

            center_end, center_start = centers_num[0], centers_num[1]  # tr√°i h∆°n = 'S·ªë cu·ªëi nƒÉm'
            left_min = float(np.percentile(data["left"], 5))
            code_right_bound = left_min + (center_note - left_min) * 0.30

    
        # m·ªü r·ªông ranh gi·ªõi ƒë·ªÉ tr√°nh ƒë·∫©y nh·∫ßm
        # ƒë·∫©y ranh gi·ªõi ph·∫£i c·ªßa "M√£ s·ªë" ra 45% kho·∫£ng c√°ch t·ªõi c·ªôt "Thuy·∫øt minh"
        # thu h·∫πp c·ªôt "M√£ s·ªë" ƒë·ªÉ ch·ªØ nh∆∞ "C√°c", "T√†i" kh√¥ng r∆°i v√†o A
        code_right_bound = left_min + (center_note - left_min) * 0.22   # 22% b·ªÅ ngang tr√°i
        item_right_bound = center_note - (center_note - code_right_bound) * 0.35
        note_right_bound = center_note + (center_end - center_note) * 0.30


        # --- 2) Gom d√≤ng ---
        lines = _group_lines(data)

        out_rows = []
        for g in lines:
            row = [""] * 5
            for _, tok in g.sort_values("left").iterrows():
                t  = str(tok["text"]).strip()
                lx = float(tok["left"])

                # ∆Øu ti√™n NG·ªÆ NGHƒ®A tr∆∞·ªõc
                if _is_code_token(t) and lx <= code_right_bound:
                    # ƒë√£ c√≥ code m√† l·∫°i g·∫∑p I., 1.1... -> ƒë∆∞a v√†o thuy·∫øt minh
                    if row[0] and _is_note_token(t):
                        row[2] = (row[2] + " " + t).strip()
                    else:
                        row[0] = (row[0] + " " + t).strip()
                    continue

                # Ti√™u ƒë·ªÅ/m·ª•c l·ªõn (I., II., A., B., ...)
                if SECTION_RE.fullmatch(t.replace(" ", "")) and lx < center_note:
                    row[1] = (row[1] + " " + t).strip()
                    continue

                if _is_note_token(t) and lx <= note_right_bound and lx >= (center_note - (center_end - center_note)*0.8):
                    row[2] = (row[2] + " " + t).strip()
                    continue

                # c·ª©u c√°nh: '5.2', '6', '7' h∆°i l·ªách ph·∫£i
                if _is_note_token(t) and lx < (center_end - (center_end - center_note)*0.35):
                    row[2] = (row[2] + " " + t).strip()
                    continue

                if _is_numeric_token(t):
                    # ch·ªçn c·ªôt s·ªë g·∫ßn t√¢m n√†o h∆°n
                    dest = 3 if abs(lx - center_end) <= abs(lx - center_start) else 4
                    row[dest] = (row[dest] + " " + t).strip()
                    continue

                # Kh√¥ng kh·ªõp ng·ªØ nghƒ©a -> d√πng v·ªã tr√≠
                if lx <= code_right_bound:
                    row[0] = (row[0] + " " + t).strip()
                elif lx <= item_right_bound:
                    row[1] = (row[1] + " " + t).strip()
                elif lx <= note_right_bound:
                    row[2] = (row[2] + " " + t).strip()
                else:
                    row[3] = (row[3] + " " + t).strip()

            out_rows.append(row)


        df = pd.DataFrame(out_rows, columns=EXPECTED_COLS)

        # --- 3) B·ªè ph·∫ßn r√°c tr∆∞·ªõc d√≤ng m√£ s·ªë ƒë·∫ßu ti√™n ---
        first_idx = None
        for i, v in enumerate(df["M√£ s·ªë"]):
            if _is_code_token(v):
                first_idx = i; break
        if first_idx is not None:
            df = df.iloc[first_idx:].reset_index(drop=True)

        # --- 4) Chu·∫©n ho√° & ghi file ---
        df = auto_postprocess_table(df)
        df = check_subtotals(df)

        df.to_csv(csv_path, index=False, encoding="utf-8-sig")
        excel_path = csv_path.replace(".csv", ".xlsx")
        with pd.ExcelWriter(excel_path, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False, sheet_name="Sheet1")
            wb = writer.book; ws = writer.sheets["Sheet1"]
            num_fmt = wb.add_format({"num_format": "#,##0"})
            ws.set_column("A:A", 9)
            ws.set_column("B:B", 60)
            ws.set_column("C:C", 12)
            ws.set_column("D:E", 22, num_fmt)

        print(f"üìÑ Fallback CSV + Excel (improved): {csv_path}, {excel_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è Fallback CSV l·ªói: {e}")


# ====== Core ======
def process_pdf(pdf_path: str, input_root: str, out_root: str,
                start_page: int, end_page: Optional[int], dpi: int,
                use_preprocess: bool, min_rows_excel: int):
    print(f"\nüìÇ ƒêang x·ª≠ l√Ω file: {pdf_path}")

    rel_path = os.path.relpath(pdf_path, input_root)
    out_dir = os.path.join(out_root, os.path.dirname(rel_path))
    ensure_dir(out_dir)

    base = os.path.splitext(os.path.basename(pdf_path))[0]

    table_engine = PPStructure(show_log=False, use_gpu=False, layout=True, recovery=True)

    pages, s_used, e_used = read_pdf_pages(pdf_path, start_page, end_page, dpi)
    print(f"üìë X·ª≠ l√Ω trang {s_used} ‚Üí {e_used} (DPI={dpi})")

    current_unit: Optional[str] = None

    for pno, pil_img in enumerate(pages, start=s_used):
        bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
        roi_bgr = detect_table_roi(bgr)
        img_for_table = preprocess_for_borderless(roi_bgr) if use_preprocess else roi_bgr

        excel_file = os.path.join(out_dir, f"{base}_page{pno}.xlsx")
        csv_file   = os.path.join(out_dir, f"{base}_page{pno}.csv")

        # 1) PPStructure tr∆∞·ªõc
        moved_excel = False
        try:
            result = table_engine(img_for_table)
            tables = []
            for el in result or []:
                if isinstance(el, dict) and el.get("type") == "table":
                    html = (el.get("res") or {}).get("html")
                    if html:
                        try:
                            dfs = pd.read_html(html)
                            if dfs: tables.append(dfs[0])
                        except Exception as ex:
                            print(f"‚ö†Ô∏è read_html fail: {ex}")
            if tables:
                df = pd.concat(tables, ignore_index=True)
                df = auto_postprocess_table(df)
                with pd.ExcelWriter(excel_file, engine="xlsxwriter") as writer:
                    df.to_excel(writer, index=False, sheet_name="Sheet1")
                    wb = writer.book; ws = writer.sheets["Sheet1"]
                    num_fmt = wb.add_format({"num_format": "#,##0"})
                    ws.set_column("A:A", 9)
                    ws.set_column("B:B", 60)
                    ws.set_column("C:C", 12)
                    ws.set_column("D:E", 22, num_fmt)
                df.to_csv(csv_file, index=False, encoding="utf-8-sig")
                moved_excel = True
        except Exception as e:
            print(f"‚ö†Ô∏è PPStructure l·ªói trang {pno}: {e}")

        # 2) Fallback n·∫øu kh√¥ng c√≥ Excel
        if not moved_excel:
            tesseract_csv_fallback(roi_bgr, csv_file)

        # 3) N·∫øu ƒë√£ c√≥ Excel ‚Üí ki·ªÉm tra s·ªë d√≤ng & xu·∫•t text ƒë·ªëi chi·∫øu
        if os.path.isfile(excel_file):
            try:
                df_check = pd.read_excel(excel_file, engine="openpyxl")
                df_check = auto_postprocess_table(df_check)
                with pd.ExcelWriter(excel_file, engine="xlsxwriter") as writer:
                    df_check.to_excel(writer, index=False, sheet_name="Sheet1")
                    wb = writer.book; ws = writer.sheets["Sheet1"]
                    num_fmt = wb.add_format({"num_format": "#,##0"})
                    ws.set_column("A:A", 9)
                    ws.set_column("B:B", 60)
                    ws.set_column("C:C", 12)
                    ws.set_column("D:E", 22, num_fmt)
                df_check.to_csv(csv_file, index=False, encoding="utf-8-sig")

                if df_check.shape[0] < min_rows_excel:
                    print(f"‚ö†Ô∏è Excel {excel_file} ch·ªâ c√≥ {df_check.shape[0]} d√≤ng (<{min_rows_excel}) ‚Üí fallback CSV.")
                    try:
                        os.remove(excel_file)
                    except Exception as e:
                        print(f"‚ö†Ô∏è Kh√¥ng xo√° ƒë∆∞·ª£c {excel_file}: {e}")
                    tesseract_csv_fallback(roi_bgr, csv_file)
                else:
                    txt = excel_to_text(excel_file, f"{base}_page{pno}")
                    if txt:
                        with open(excel_file.replace(".xlsx", "_excel.txt"), "w", encoding="utf-8") as f:
                            f.write(txt)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói Excel {excel_file}: {e}")
                tesseract_csv_fallback(roi_bgr, csv_file)

        # 4) Text to√†n trang (kh√¥ng crop) + metadata
        try:
            txt_tess = pytesseract.image_to_string(bgr, lang="eng+vie")
        except pytesseract.TesseractNotFoundError:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Tesseract. Set TESSERACT_CMD t·ªõi tesseract.exe.")
            txt_tess = ""
        with open(os.path.join(out_dir, f"{base}_page{pno}_text.txt"), "w", encoding="utf-8") as f:
            f.write(txt_tess)

        unit_found = detect_unit(txt_tess)
        if unit_found:
            current_unit = unit_found

        meta = {
            "file": base, "page": pno, "unit": current_unit,
            "source_pdf": os.path.abspath(pdf_path), "dpi": dpi,
            "preprocess": use_preprocess,
        }
        with open(os.path.join(out_dir, f"{base}_page{pno}_meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

# ====== Runner ======
def run_ocr_pipeline(input_file: Optional[str] = INPUT_FILE_DEFAULT,
                     input_dir: str = INPUT_DIR_DEFAULT,
                     out_dir: str = OUTPUT_DIR_DEFAULT,
                     start_page: int = 1,
                     end_page: Optional[int] = None,
                     dpi: int = DPI_DEFAULT,
                     use_preprocess: bool = True,
                     force_clean_out: bool = True,
                     min_rows_excel: int = MIN_ROWS_DEFAULT):
    # H·ªèi ng∆∞·ªùi d√πng n·∫øu output ƒë√£ t·ªìn t·∫°i
    if os.path.exists(out_dir):
        choice = input(f"‚ö†Ô∏è Output {out_dir} ƒë√£ t·ªìn t·∫°i. "
                       "Ch·ªçn y=xo√° build l·∫°i, a=append th√™m file m·ªõi, n=b·ªè qua: ").strip().lower()
        if choice == "y":
            shutil.rmtree(out_dir, ignore_errors=True); print(f"üóëÔ∏è ƒê√£ xo√° {out_dir}")
        elif choice == "n":
            print("‚è≠Ô∏è B·ªè qua OCR pipeline."); return
        elif choice == "a":
            print(f"‚ûï Gi·ªØ {out_dir}, OCR th√™m file m·ªõi.")
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, b·ªè qua."); return
    ensure_dir(out_dir)

    if input_file:
        if not os.path.isfile(input_file):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {input_file}"); return
        process_pdf(input_file,
                    os.path.dirname(input_file) if os.path.isdir(input_dir) else input_dir,
                    out_dir, start_page, end_page, dpi, use_preprocess, min_rows_excel)
    else:
        if not os.path.isdir(input_dir):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {input_dir}"); return
        pdf_files = glob.glob(os.path.join(input_dir, "**", "*.pdf"), recursive=True)
        if not pdf_files:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y PDF n√†o."); return
        for pdf in pdf_files:
            process_pdf(pdf, input_dir, out_dir, start_page, end_page, dpi, use_preprocess, min_rows_excel)

def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser("OCR pipeline (PPStructure + Tesseract) cho PDF scan")
    src = p.add_mutually_exclusive_group(required=False)
    src.add_argument("--file", type=str, help="ƒê∆∞·ªùng d·∫´n PDF c·∫ßn OCR")
    src.add_argument("--dir", type=str, help="Th∆∞ m·ª•c ch·ª©a PDF (recursive)")
    p.add_argument("--out", type=str, default=OUTPUT_DIR_DEFAULT, help="Th∆∞ m·ª•c output")
    p.add_argument("--start", type=int, default=1, help="Trang b·∫Øt ƒë·∫ßu (1-based)")
    p.add_argument("--end", type=int, default=None, help="Trang k·∫øt th√∫c (1-based, inclusive)")
    p.add_argument("--dpi", type=int, default=DPI_DEFAULT, help="DPI render ·∫£nh t·ª´ PDF")
    p.add_argument("--no-pre", action="store_true", help="T·∫Øt ti·ªÅn x·ª≠ l√Ω ·∫£nh (m·∫∑c ƒë·ªãnh b·∫≠t)")
    p.add_argument("--keep-out", action="store_true", help="Kh√¥ng xo√° output c≈© (m·∫∑c ƒë·ªãnh xo√°)")
    p.add_argument("--min-rows", type=int, default=MIN_ROWS_DEFAULT, help="Excel < min-rows ‚Üí fallback CSV")
    return p

if __name__ == "__main__":
    args = build_argparser().parse_args()
    run_ocr_pipeline(
        input_file=args.file,
        input_dir=args.dir if args.dir else INPUT_DIR_DEFAULT,
        out_dir=args.out,
        start_page=args.start,
        end_page=args.end,
        dpi=args.dpi,
        use_preprocess=(not args.no_pre),
        force_clean_out=(not args.keep_out),
        min_rows_excel=args.min_rows,
    )
