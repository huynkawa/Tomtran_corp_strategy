# -*- coding: utf-8 -*-
"""
a2_ocr_mixed_text_table_GPT.py
---------------------------------------
ƒê·ªçc & x·ª≠ l√Ω t√†i li·ªáu h·ªón h·ª£p (DOCX / PDF / IMG / TXT / Excel):
- T·ª± nh·∫≠n d·∫°ng lo·∫°i file, OCR khi c·∫ßn (x·ª≠ l√Ω RAM, kh√¥ng l∆∞u .png)
- PDF √°p d·ª•ng ƒë∆∞·ª£c ph·∫°m vi TRANG (--start/--end) th·ª±c s·ª±
- Gi·ªØ c·∫•u tr√∫c block r√µ r√†ng: [DOCX]/[PDF]/[EXCEL] + [PARA]/[TABLE]/[SHEET]
- B·∫¢NG xu·∫•t theo TSV (tab '\t') ƒë·ªÉ "vector-ready"
- GPT tham gia theo YAML:
    + table_only: ch·ªâ chu·∫©n ho√° b·∫£ng (kh√¥ng b·ªãa s·ªë)
    + paragraph_with_headings: chu·∫©n ho√° ti√™u ƒë·ªÅ, KH√îNG di·ªÖn gi·∫£i
- Auto-fix heading nh·∫π d·ª±a tr√™n YAML (n·∫øu c√≥ heading_patterns)
- Xu·∫•t: <name>_text.txt + <name>_meta.json
  (tu·ª≥ ch·ªçn: --vector-jsonl ƒë·ªÉ xu·∫•t th√™m <name>_vector.jsonl)

Author: TOMTRAN (revised, portable & structured)
"""

import os, re, io, json, hashlib, argparse, sys
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import pdfplumber
from PIL import Image, ImageOps
import pytesseract
from docx import Document
from docx.table import _Cell, Table
from docx.text.paragraph import Paragraph
from tqdm import tqdm
from langdetect import detect
import yaml
import pandas as pd

# ====== ENV / GPT enhancer ======
# L∆∞u √Ω: module src.env s·∫Ω n·∫°p OPENAI_API_KEY t·ª´ .env.active n·∫øu c√≥
try:
    import src.env  # noqa: F401
except Exception:
    pass

# 1 ƒëi·ªÉm import duy nh·∫•t (tr√°nh tr√πng l·∫∑p)
try:
    from src.gpt_enhancer import enhance_with_gpt as _enhance_with_gpt
except Exception:
    def _enhance_with_gpt(text, meta=None, image=None, **kwargs):
        # fallback "an to√†n": kh√¥ng d√πng GPT
        return text

# ====== C·∫•u h√¨nh m·∫∑c ƒë·ªãnh (portable, c√≥ th·ªÉ override b·∫±ng CLI) ======
OCR_LANG_DEFAULT = "vie+eng"
OCR_PSM_DEFAULT = "6"  # 6 = Assume a single uniform block of text
OCR_CFG_TEMPLATE = "--psm {psm} preserve_interword_spaces=1"

# M·∫∑c ƒë·ªãnh portable (kh√¥ng hard-code D:\). C√≥ th·ªÉ override b·∫±ng CLI.

INPUT_DIR  = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\inputs\a_text_only_inputs_test"
OUTPUT_DIR = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\a1_ocr_mixed_text_table_GPT"
YAML_RULE_PATH= r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\configs\a1_text_only_rules.yaml"

# ====== Ti·ªán √≠ch chung ======
def sha1_of_text(text: str) -> str:
    return hashlib.sha1((text or "").encode("utf-8")).hexdigest()

def detect_language_safe(text: str) -> str:
    try:
        return detect(text or "")
    except Exception:
        return "unknown"

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def load_yaml_rules(path: str) -> dict:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except Exception:
        return {}

def match_yaml_meta(file_name: str, rules: dict) -> dict:
    """
    T√¨m metadata m·∫∑c ƒë·ªãnh theo regex t·ª´ YAML:
    rules:
      defaults: {...}
      files:
        - match: "regex"
          company: "UIC"
          appendix_id: "App 21"
          ...
    """
    if not rules:
        return {}
    defaults = rules.get("defaults", {}) or {}
    for r in rules.get("files", []) or []:
        patt = r.get("match")
        if patt and re.search(patt, file_name, flags=re.I):
            one = r.copy()
            one.pop("match", None)
            return {**defaults, **one}
    return defaults

def build_gpt_prompt_from_yaml(yaml_rules: dict, mode: str) -> str:
    """
    K·∫øt h·ª£p prompt t·ª´ YAML theo mode:
      - table_only
      - paragraph_with_headings
    """
    prompt = []
    gp = (yaml_rules or {}).get("gpt_prompt") or {}
    # prompt chung
    if gp.get("common"):
        prompt.append(str(gp["common"]).strip())
    # prompt theo mode
    if mode and gp.get(mode):
        prompt.append(str(gp[mode]).strip())

    # R√†ng bu·ªôc an to√†n
    policy = (yaml_rules or {}).get("policy") or {}
    if policy.get("no_hallucination", True):
        prompt.append("Tuy·ªát ƒë·ªëi kh√¥ng b·ªãa ƒë·∫∑t n·ªôi dung/s·ªë li·ªáu; kh√¥ng suy di·ªÖn ngo√†i vƒÉn b·∫£n g·ªëc.")
    if policy.get("keep_units", True):
        prompt.append("Kh√¥ng t·ª± ƒë·ªïi ƒë∆°n v·ªã; gi·ªØ nguy√™n ƒë∆°n v·ªã v√† s·ªë li·ªáu nh∆∞ g·ªëc.")
    if policy.get("no_translation", True):
        prompt.append("Kh√¥ng d·ªãch thu·∫≠t ng·ªØ chuy√™n ng√†nh; gi·ªØ nguy√™n ng√¥n ng·ªØ g·ªëc.")

    # Chu·∫©n TSV cho b·∫£ng
    if mode == "table_only":
        prompt.append("ƒê·∫ßu ra B·∫¢NG theo TSV: m·ªói √¥ c√°ch nhau b·∫±ng tab (\\t), m·ªói h√†ng m·ªôt d√≤ng. Kh√¥ng th√™m m√¥ t·∫£.")
    # Paragraph
    if mode == "paragraph_with_headings":
        prompt.append("Ch·ªâ CHU·∫®N HO√Å ti√™u ƒë·ªÅ/heading & d√†n √Ω; kh√¥ng di·ªÖn gi·∫£i th√™m n·ªôi dung.")

    return "\n".join([p for p in prompt if p]).strip()

def normalize_to_tsv(rows: List[List[str]]) -> str:
    """
    Nh·∫≠n list 2D v√† tr·∫£ v·ªÅ TSV (tab-delimited).
    """
    out_lines = []
    for row in rows:
        safe_cells = [(str(c) if c is not None else "").strip() for c in row]
        out_lines.append("\t".join(safe_cells))
    return "\n".join(out_lines)

def is_tableish_line(line: str) -> bool:
    """
    Heuristic: 1 d√≤ng c√≥ nhi·ªÅu kho·∫£ng c√°ch/c·ªôt -> coi l√† d√≤ng b·∫£ng.
    """
    # C√≥ nhi·ªÅu tab ho·∫∑c c√≥ '|' ho·∫∑c c√≥ >= 3 nh√≥m kho·∫£ng tr·∫Øng d√†i
    return ("\t" in line) or ("|" in line) or (len(re.findall(r"\s{2,}", line)) >= 2)

def split_text_into_text_vs_table_blocks(text: str) -> List[Dict[str, str]]:
    """
    T·ª´ text layer PDF: t√°ch block TABLE vs TEXT th√¥ b·∫±ng heuristic (m·ªÅm).
    """
    blocks, buf, buf_is_table = [], [], None
    for raw in (text or "").splitlines():
        line = raw.rstrip()
        line_is_table = is_tableish_line(line)
        if buf_is_table is None:
            buf_is_table = line_is_table
            buf = [line]
        elif line_is_table == buf_is_table:
            buf.append(line)
        else:
            blocks.append({"type": "table" if buf_is_table else "paragraph",
                           "content": "\n".join(buf).strip()})
            buf = [line]
            buf_is_table = line_is_table
    if buf:
        blocks.append({"type": "table" if buf_is_table else "paragraph",
                       "content": "\n".join(buf).strip()})
    return blocks

# ====== DOCX ======
def read_docx_paragraphs_and_tables(file_path: Path) -> List[Dict[str, str]]:
    """
    ƒê·ªçc DOCX v·ªõi c·∫•p cao:
    - paragraphs: l·∫•y style.name ƒë·ªÉ suy ra Heading
    - tables: l·∫•y t·∫•t c·∫£ cell theo h√†ng/c·ªôt -> TSV
    """
    doc = Document(file_path)
    results: List[Dict[str, str]] = []

    # 1) Paragraphs
    for para in doc.paragraphs:
        text = (para.text or "").strip()
        if not text:
            continue
        style_name = getattr(para.style, "name", "") or ""
        # ƒê∆∞a heading ra tr∆∞·ªõc n·ªôi dung (gi·ªØ l√†m m·ªëc)
        if style_name.lower().startswith("heading"):
            results.append({"type": "paragraph", "content": f"{text}"})
        else:
            results.append({"type": "paragraph", "content": text})

    # 2) Tables
    for t in doc.tables:
        rows2d: List[List[str]] = []
        for r in t.rows:
            row_cells = []
            for c in r.cells:
                row_cells.append((c.text or "").strip())
            # m·ªôt s·ªë DOCX l·∫∑p l·∫°i cell do merge; lo·∫°i b·ªè tr√πng li√™n ti·∫øp
            dedup = []
            for i, val in enumerate(row_cells):
                if i == 0 or val != row_cells[i-1]:
                    dedup.append(val)
            rows2d.append(dedup)
        table_tsv = normalize_to_tsv(rows2d)
        results.append({"type": "table", "content": table_tsv})

    return results

# ====== PDF ======
def preprocess_pil_for_ocr(img: Image.Image) -> Image.Image:
    """
    Ti·ªÅn x·ª≠ l√Ω ƒë∆°n gi·∫£n tr∆∞·ªõc khi OCR: chuy·ªÉn L + autocontrast.
    (Tr√°nh OpenCV ƒë·ªÉ gi·∫£m ph·ª• thu·ªôc.)
    """
    gray = img.convert("L")
    gray = ImageOps.autocontrast(gray)
    return gray

def read_pdf_text_and_images(file_path: Path, page_start: int = 1, page_end: Optional[int] = None,
                             ocr_lang: str = OCR_LANG_DEFAULT, ocr_psm: str = OCR_PSM_DEFAULT) -> List[Dict[str, str]]:
    """
    ƒê·ªçc PDF: √°p d·ª•ng ph·∫°m vi trang, t√°ch TEXT layer v√† OCR ·∫£nh.
    - TEXT: t√°ch block paragraph/table nh·∫π b·∫±ng heuristic
    - IMG : OCR -> coi l√† table block (ƒë·ªÉ GPT chu·∫©n ho√° b·∫£ng)
    """
    results: List[Dict[str, str]] = []
    if page_start < 1:
        page_start = 1
    try:
        with pdfplumber.open(file_path) as pdf:
            N = len(pdf.pages)
            if not page_end or page_end > N:
                page_end = N
            # 1-based inclusive -> zero-based slice
            for idx in range(page_start - 1, page_end):
                page = pdf.pages[idx]
                text = page.extract_text() or ""
                if text.strip():
                    # t√°ch block TABLE/TEXT d·ª±a heuristic
                    for b in split_text_into_text_vs_table_blocks(text):
                        # nh√£n ngu·ªìn trang
                        btype = b["type"]
                        content = b["content"]
                        if content:
                            results.append({
                                "type": btype,
                                "content": content,
                                "page": idx + 1
                            })
                # OCR ·∫£nh (m·ªói ·∫£nh -> table block)
                for im in page.images or []:
                    x0, top, x1, bottom = im["x0"], im["top"], im["x1"], im["bottom"]
                    bbox = (x0, top, x1, bottom)
                    cropped_page = page.crop(bbox)
                    try:
                        pil_img = cropped_page.to_image(resolution=300).original  # PIL
                        pil_img = preprocess_pil_for_ocr(pil_img)
                        cfg = OCR_CFG_TEMPLATE.format(psm=ocr_psm)
                        txt = pytesseract.image_to_string(pil_img, lang=ocr_lang, config=cfg)
                        if (txt or "").strip():
                            results.append({
                                "type": "table",
                                "content": txt.strip(),
                                "page": idx + 1,
                                "source": "image_ocr"
                            })
                    except Exception as e:
                        print(f"‚ö†Ô∏è OCR ·∫£nh l·ªói (page {idx+1}): {e}")
    except Exception as e:
        print(f"‚ö†Ô∏è PDF ƒë·ªçc l·ªói {file_path}: {e}")
    return results

# ====== IMG ======
def ocr_image_to_text(file_path: Path, ocr_lang: str, ocr_psm: str) -> str:
    img = Image.open(file_path)
    pil = preprocess_pil_for_ocr(img)
    cfg = OCR_CFG_TEMPLATE.format(psm=ocr_psm)
    return pytesseract.image_to_string(pil, lang=ocr_lang, config=cfg)

# ====== TXT ======
def read_txt(file_path: Path) -> str:
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

# ====== Excel ======
def read_excel_as_tsv_blocks(file_path: Path) -> List[Dict[str, str]]:
    """
    M·ªói sheet -> TSV (tab-delimited), gi·ªØ header n·∫øu c√≥.
    """
    results = []
    try:
        sheets = pd.read_excel(file_path, sheet_name=None)  # header auto-detect
        for sheet_name, df in sheets.items():
            # ƒê·∫£m b·∫£o chu·ªói
            df = df.astype(object).where(pd.notna(df), "")
            # Xu·∫•t TSV
            rows2d: List[List[str]] = [list(map(lambda x: str(x).strip(), df.columns.tolist()))]
            for _, row in df.iterrows():
                rows2d.append([str(c).strip() for c in row.tolist()])
            tsv = normalize_to_tsv(rows2d)
            results.append({"sheet": str(sheet_name), "content": tsv})
    except Exception as e:
        print(f"‚ö†Ô∏è Excel ƒë·ªçc l·ªói {file_path}: {e}")
    return results

# ====== Heading auto-fix (nh·∫π) ======
def apply_heading_autofix(text: str, yaml_rules: dict) -> str:
    """
    V√° heading nh·∫π n·∫øu YAML c√≥ 'heading_patterns':
      heading_patterns:
        "SECTION D: REFERRAL RISKS": "(?i)\\breferral risks?\\b"
        "APPENDIX I_CFE TARIFF AND TABLE OF CATEGORY": "(?i)\\bCFE\\s+tariff\\b"
    √ù t∆∞·ªüng:
      - N·∫øu t√¨m th·∫•y pattern n·ªôi dung nh∆∞ng kh√¥ng c√≥ heading chu·∫©n -> ch√®n heading tr∆∞·ªõc d√≤ng ƒë·∫ßu ti√™n kh·ªõp.
    """
    hp = (yaml_rules or {}).get("heading_patterns") or {}
    if not hp:
        return text
    text_norm = text  # s·∫Ω thao t√°c tr·ª±c ti·∫øp
    for heading, patt in hp.items():
        try:
            # ƒë√£ c√≥ heading chu·∫©n?
            if re.search(re.escape(heading), text_norm, flags=re.I):
                continue
            m = re.search(patt, text_norm, flags=re.I | re.M)
            if m:
                # Ch√®n heading tr∆∞·ªõc d√≤ng n∆°i pattern xu·∫•t hi·ªán
                start = m.start()
                # T√¨m ƒë·∫ßu d√≤ng
                line_start = text_norm.rfind("\n", 0, start)
                if line_start == -1:
                    # ch√®n ƒë·∫ßu file
                    text_norm = f"{heading}\n{text_norm}"
                else:
                    insert_pos = line_start + 1
                    text_norm = text_norm[:insert_pos] + f"{heading}\n" + text_norm[insert_pos:]
        except re.error:
            # regex l·ªói -> b·ªè qua m·ª•c n√†y
            continue
    return text_norm

# ====== Vector JSONL (t√πy ch·ªçn) ======
def append_vector_jsonl(vec_path: Path, content: str, metadata: dict):
    with open(vec_path, "a", encoding="utf-8") as f:
        rec = {"content": content, "metadata": metadata}
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

# ====== X·ª≠ l√Ω ch√≠nh 1 file ======
def process_file(file_path: Path,
                 yaml_rules: dict,
                 out_dir: Path,
                 ocr_lang: str,
                 ocr_psm: str,
                 page_start: int,
                 page_end: Optional[int],
                 vector_jsonl: bool = False) -> None:
    file_name = file_path.stem
    ext = file_path.suffix.lower()

    # Meta g·ªëc
    meta: Dict = {
        "file": file_name,
        "source_path": str(file_path.resolve()),
        "ocr_lang": ocr_lang,
        "ocr_psm": ocr_psm,
    }
    meta.update(match_yaml_meta(file_name, yaml_rules))

    combined_lines: List[str] = []
    has_table = False
    gpt_applied = False
    gpt_reasons: List[str] = []
    vector_path = out_dir / f"{file_name}_vector.jsonl" if vector_jsonl else None

    # Helper g·ªçi GPT v·ªõi prompt d·ª±a YAML
    def call_gpt(text: str, mode: str, extra_meta: dict) -> str:
        nonlocal gpt_applied, gpt_reasons
        prompt = build_gpt_prompt_from_yaml(yaml_rules, mode)
        gpt_applied = True
        reason = f"{mode}"
        if extra_meta.get("source"):
            reason += f":{extra_meta['source']}"
        if reason not in gpt_reasons:
            gpt_reasons.append(reason)
        # truy·ªÅn prompt & mode ƒë·ªÉ enhancer bi·∫øt
        return _enhance_with_gpt(text, {**meta, **extra_meta, "gpt_mode": mode}, None, prompt=prompt, mode=mode)

    try:
        if ext == ".docx":
            blocks = read_docx_paragraphs_and_tables(file_path)
            # DOCX: block theo th·ª© t·ª± ‚Äì para/table
            for idx, b in enumerate(blocks, 1):
                if b["type"] == "table":
                    has_table = True
                    # TSV ƒë√£ chu·∫©n -> v·∫´n cho GPT "table_only" ƒë·ªÉ chu·∫©n ho√°/kh·ª≠ r√°c
                    enhanced = call_gpt(b["content"], mode="table_only", extra_meta={"doc_type": "DOCX", "table_index": idx})
                    combined_lines.append(f"### [DOCX] [TABLE {idx}]")
                    combined_lines.append(enhanced.strip())
                    if vector_path:
                        append_vector_jsonl(vector_path, enhanced.strip(),
                                            {**meta, "content_type": "TABLE", "doc_type": "DOCX", "table_index": idx})
                else:
                    # Paragraph: cho GPT chu·∫©n ho√° heading n·∫øu mu·ªën (t√πy YAML)
                    para = b["content"]
                    if (yaml_rules or {}).get("enable_paragraph_gpt", True):
                        enhanced = call_gpt(para, mode="paragraph_with_headings", extra_meta={"doc_type": "DOCX"})
                    else:
                        enhanced = para
                    combined_lines.append(f"### [DOCX] [PARA]")
                    combined_lines.append(enhanced.strip())
                    if vector_path:
                        append_vector_jsonl(vector_path, enhanced.strip(),
                                            {**meta, "content_type": "TEXT", "doc_type": "DOCX"})

        elif ext == ".pdf":
            blocks = read_pdf_text_and_images(file_path, page_start=page_start, page_end=page_end,
                                              ocr_lang=ocr_lang, ocr_psm=ocr_psm)
            # PDF: block c√≥ page + type
            page_table_count: Dict[int, int] = {}
            for b in blocks:
                btype = b.get("type")
                page_no = b.get("page", 0)
                if btype == "table":
                    has_table = True
                    page_table_count[page_no] = page_table_count.get(page_no, 0) + 1
                    idx_on_page = page_table_count[page_no]
                    enhanced = call_gpt(b["content"], mode="table_only",
                                        extra_meta={"doc_type": "PDF", "page": page_no, "table_index": idx_on_page, "source": b.get("source", "text_layer")})
                    combined_lines.append(f"### [PDF page {page_no}] [TABLE {idx_on_page}]")
                    combined_lines.append(enhanced.strip())
                    if vector_path:
                        append_vector_jsonl(vector_path, enhanced.strip(),
                                            {**meta, "content_type": "TABLE", "doc_type": "PDF",
                                             "page": page_no, "table_index": idx_on_page})
                else:
                    # Paragraph: c√≥ th·ªÉ ch·∫°y GPT normalize heading (t√πy)
                    para = b.get("content", "")
                    if (yaml_rules or {}).get("enable_paragraph_gpt", True):
                        enhanced = call_gpt(para, mode="paragraph_with_headings",
                                            extra_meta={"doc_type": "PDF", "page": page_no})
                    else:
                        enhanced = para
                    combined_lines.append(f"### [PDF page {page_no}] [TEXT]")
                    combined_lines.append(enhanced.strip())
                    if vector_path:
                        append_vector_jsonl(vector_path, enhanced.strip(),
                                            {**meta, "content_type": "TEXT", "doc_type": "PDF", "page": page_no})

        elif ext in [".jpg", ".jpeg", ".png", ".tif", ".tiff", ".bmp"]:
            # IMG -> OCR r·ªìi GPT
            txt = ocr_image_to_text(file_path, ocr_lang=ocr_lang, ocr_psm=ocr_psm).strip()
            if txt:
                has_table = True
                enhanced = call_gpt(txt, mode="table_only", extra_meta={"doc_type": "IMG"})
                combined_lines.append(f"### [IMG] [TABLE 1]")
                combined_lines.append(enhanced.strip())
                if vector_path:
                    append_vector_jsonl(vector_path, enhanced.strip(),
                                        {**meta, "content_type": "TABLE", "doc_type": "IMG"})

        elif ext in [".txt", ".csv"]:
            raw = read_txt(file_path).strip()
            # TXT -> ƒë·ªÉ nguy√™n, c√≥ th·ªÉ normalize heading n·∫øu b·∫≠t
            if (yaml_rules or {}).get("enable_paragraph_gpt", False):
                enhanced = _enhance_with_gpt(raw, {**meta, "gpt_mode": "paragraph_with_headings"}, None,
                                             prompt=build_gpt_prompt_from_yaml(yaml_rules, "paragraph_with_headings"),
                                             mode="paragraph_with_headings")
                gpt_applied = True
                gpt_reasons.append("paragraph_with_headings")
            else:
                enhanced = raw
            combined_lines.append(f"### [TXT] [TEXT]")
            combined_lines.append(enhanced.strip())
            if vector_path:
                append_vector_jsonl(vector_path, enhanced.strip(),
                                    {**meta, "content_type": "TEXT", "doc_type": "TXT"})

        elif ext in [".xlsx", ".xls"]:
            excel_blocks = read_excel_as_tsv_blocks(file_path)
            for i, b in enumerate(excel_blocks, 1):
                has_table = True
                # Cho GPT table_only ƒë·ªÉ gi·ªØ TSV s·∫°ch
                enhanced = call_gpt(b["content"], mode="table_only",
                                    extra_meta={"doc_type": "EXCEL", "sheet": b["sheet"]})
                combined_lines.append(f"### [EXCEL] [SHEET={b['sheet']}] [TABLE {i}]")
                combined_lines.append(enhanced.strip())
                if vector_path:
                    append_vector_jsonl(vector_path, enhanced.strip(),
                                        {**meta, "content_type": "TABLE", "doc_type": "EXCEL", "sheet": b["sheet"], "table_index": i})

        else:
            print(f"‚ö†Ô∏è Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng {ext} ‚Äî b·ªè qua: {file_path.name}")
            return

        combined_text = "\n".join(combined_lines).strip()

        # Auto-fix heading nh·∫π n·∫øu YAML c√≥ heading_patterns
        combined_text = apply_heading_autofix(combined_text, yaml_rules)

        # Enrich meta
        lang = detect_language_safe(combined_text)
        meta.update({
            "language": lang,
            "has_table": has_table,
            "gpt_applied": gpt_applied,
            "gpt_reasons": sorted(set(gpt_reasons)),
            "text_sha1": sha1_of_text(combined_text),
            "rules_version": (yaml_rules.get("version") if yaml_rules else None),
            "page_range_applied": {"start": page_start, "end": page_end} if page_start or page_end else None
        })

        # Ghi output
        txt_out = out_dir / f"{file_name}_text.txt"
        meta_out = out_dir / f"{file_name}_meta.json"
        with open(txt_out, "w", encoding="utf-8") as f:
            f.write(combined_text)
        with open(meta_out, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ Saved: {txt_out.name}, {meta_out.name}{' + vector.jsonl' if vector_path else ''}")

    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω {file_path.name}: {e}")

# ====== CLI ======
def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="A2 ‚Äî OCR + GPT cho t√†i li·ªáu h·ªón h·ª£p (text + b·∫£ng + ·∫£nh), xu·∫•t vector-ready TSV.")
    p.add_argument("--input", default=INPUT_DIR, help="Th∆∞ m·ª•c input (m·∫∑c ƒë·ªãnh: ./inputs)")
    p.add_argument("--out", default=OUTPUT_DIR, help="Th∆∞ m·ª•c output (m·∫∑c ƒë·ªãnh: ./outputs/a2_ocr_mixed_text_table_GPT)")
    p.add_argument("--yaml", default=YAML_RULE_PATH, help="ƒê∆∞·ªùng d·∫´n YAML rule (m·∫∑c ƒë·ªãnh: ./configs/a1_text_only_rules.yaml)")
    p.add_argument("--mode", choices=["pages", "files"], default="pages",
                   help="pages = --start/--end l√† ph·∫°m vi TRANG cho PDF; files = theo ch·ªâ s·ªë FILE")
    p.add_argument("--start", type=int, default=1, help="[pages] TRANG b·∫Øt ƒë·∫ßu (1-based)")
    p.add_argument("--end", type=int, default=None, help="[pages] TRANG k·∫øt th√∫c (1-based, inclusive). N·∫øu b·ªè tr·ªëng -> ƒë·∫øn h·∫øt")
    p.add_argument("--file_index", type=int, default=None,
                   help="[pages] Ch·ªâ ch·∫°y 1 PDF theo th·ª© t·ª± (1-based). B·ªè qua ƒë·ªÉ ch·∫°y t·∫•t c·∫£ PDF. Kh√¥ng ·∫£nh h∆∞·ªüng non-PDF.")
    p.add_argument("--ocr-lang", default=OCR_LANG_DEFAULT, help="Ng√¥n ng·ªØ OCR cho ·∫£nh/PDF image (vd: vie+eng)")
    p.add_argument("--ocr-psm", default=OCR_PSM_DEFAULT, help="Tesseract PSM (vd: 4/6/11...)")
    p.add_argument("--vector-jsonl", action="store_true", help="Xu·∫•t th√™m <name>_vector.jsonl (m·ªói block 1 d√≤ng)")
    return p

def main():
    args = build_argparser().parse_args()

    input_dir = Path(args.input).resolve()
    out_dir   = Path(args.out).resolve()
    ensure_dir(out_dir)

    yaml_rules = load_yaml_rules(args.yaml)
    files = sorted(list(input_dir.rglob("*.*")))

    print("=== C·∫§U H√åNH A2 (revised) ===")
    print(f"üìÇ INPUT_DIR   : {input_dir}")
    print(f"üì¶ OUTPUT_DIR  : {out_dir}")
    print(f"‚öôÔ∏è YAML_RULE   : {args.yaml}  | loaded={'OK' if yaml_rules else 'empty'}")
    print(f"üß† MODE        : {args.mode}")
    if args.mode == "pages":
        print(f"üìÑ PAGES       : {args.start} ‚Üí {args.end or 'END'}")
        print(f"üî¢ file_index  : {args.file_index or 'ALL PDFs'}")
    else:
        print(f"üî¢ FILE index  : {args.start} ‚Üí {args.end or len(files)}")
    print(f"üî§ OCR_LANG    : {args.ocr_lang} | PSM={args.ocr_psm}")
    print(f"üßæ VECTOR.JSONL: {'ON' if args.vector_jsonl else 'OFF'}")
    print("=============================")

    if args.mode == "pages":
        pdfs = [Path(f) for f in files if Path(f).suffix.lower() == ".pdf"]
        if args.file_index and args.file_index > 0 and args.file_index <= len(pdfs):
            target = pdfs[args.file_index - 1]
            pdfs = [target]
        elif args.file_index and args.file_index > len(pdfs):
            print(f"‚ö†Ô∏è file_index={args.file_index} > s·ªë PDF ({len(pdfs)}). S·∫Ω ch·∫°y to√†n b·ªô PDF.")

        # 1) PDF theo range trang
        for fpath in tqdm(pdfs, desc="Processing PDFs by pages"):
            process_file(
                file_path=Path(fpath),
                yaml_rules=yaml_rules,
                out_dir=out_dir,
                ocr_lang=args.ocr_lang,
                ocr_psm=args.ocr_psm,
                page_start=args.start,
                page_end=args.end,
                vector_jsonl=args.vector_jsonl
            )
        # 2) Non-PDF full
        others = [Path(f) for f in files if Path(f).suffix.lower() != ".pdf"]
        if others:
            for fpath in tqdm(others, desc="Processing non-PDFs full"):
                process_file(
                    file_path=Path(fpath),
                    yaml_rules=yaml_rules,
                    out_dir=out_dir,
                    ocr_lang=args.ocr_lang,
                    ocr_psm=args.ocr_psm,
                    page_start=1,
                    page_end=None,
                    vector_jsonl=args.vector_jsonl
                )
    else:
        # MODE=files: --start/--end l√† ch·ªâ s·ªë FILE
        start = args.start if args.start and args.start > 0 else 1
        end = args.end if args.end else len(files)
        selected = files[start - 1: end]
        for fpath in tqdm(selected, desc="Processing by file index"):
            process_file(
                file_path=Path(fpath),
                yaml_rules=yaml_rules,
                out_dir=out_dir,
                ocr_lang=args.ocr_lang,
                ocr_psm=args.ocr_psm,
                page_start=1,
                page_end=None,
                vector_jsonl=args.vector_jsonl
            )
    print("üéØ Ho√†n t·∫•t A2 (revised). Ki·ªÉm tra th∆∞ m·ª•c output.")
    
if __name__ == "__main__":
    main()
