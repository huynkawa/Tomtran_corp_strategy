# -*- coding: utf-8 -*-
"""
src.b2_mix_text_table_prevector.py
c2_clean_tables_pre_vector.py ‚Äî B·∫¢N CHUY√äN CHO B·∫¢NG
- Qu√©t c·∫£ th∆∞ m·ª•c input (mirror sang out-root)
- Ch·ªâ x·ª≠ l√Ω c√°c block [TABLE ...] trong *_text.txt|*_text.final.txt
- Clean TSV m·∫°nh tay + xu·∫•t <stem>_tables.final.tsv v√† <stem>_vector.jsonl

Ch·∫°y v√≠ d·ª•:
  python -m src.c2_clean_tables_pre_vector
  # ho·∫∑c ch·ªâ x·ª≠ l√Ω file m·ªõi:
  python -m src.c2_clean_tables_pre_vector --skip-existing
"""

import os, re, json, argparse
from pathlib import Path
from typing import List, Tuple, Optional
from collections import Counter

IN_ROOT_DEFAULT  = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\b1_mix_text_table_output"
OUT_ROOT_DEFAULT = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\b2_mix_text_table_prevector"
PATTERN_DEFAULT  = "*_text.txt|*_text.final.txt"

HDR_BRACKETS = re.compile(r"\[([^\]]+)\]")
IS_HEADER    = re.compile(r"^###\s*(\[[^\]]+\]\s*)+", re.I)

def parse_header(line: str) -> dict:
    groups = re.findall(HDR_BRACKETS, line)
    meta = {
        "doc_type": None, "page": None, "sheet": None,
        "table_index": None, "content_type_hint": None,
        "raw_header": line.strip()
    }
    for g in groups:
        g_up = g.upper()
        if "TABLE" in g_up:
            meta["content_type_hint"] = "TABLE"
            m = re.search(r"TABLE\s+(\d+)", g_up)
            if m: meta["table_index"] = int(m.group(1))
            continue
        if "TEXT" in g_up:
            meta["content_type_hint"] = "TEXT"; continue
        if g_up.startswith("PDF"):
            meta["doc_type"] = "PDF"
            m = re.search(r"PAGE\s+(\d+)", g_up)
            if m: meta["page"] = int(m.group(1)); continue
        if g_up.startswith("DOCX"):  meta["doc_type"] = "DOCX";  continue
        if g_up.startswith("IMG"):   meta["doc_type"] = "IMG";   continue
        if g_up.startswith("TXT"):   meta["doc_type"] = "TXT";   continue
        if g_up.startswith("EXCEL"): meta["doc_type"] = "EXCEL"; continue
        if g_up.startswith("SHEET="): meta["sheet"] = g.split("=",1)[-1]; continue
    return meta

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def load_base_meta(input_txt_path: Path) -> dict:
    meta_path = Path(str(input_txt_path).replace("_text.final.txt", "_meta.json"))
    if not meta_path.exists():
        meta_path = Path(str(input_txt_path).replace("_text.txt", "_meta.json"))
    if meta_path.exists():
        try:
            return json.loads(meta_path.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

# ===== C√°c quy t·∫Øc clean cho TABLE =====
NOISY_ROW_RE = re.compile(
    r"^(GN·ªòƒêC√ÅT·ª∞S|KH·∫¢ NƒÇNG X·∫¢Y RA\b|\d+\s*-\s*Kh√¥ng\s*$|^Trang\s*\d+\s*$|^Page\s*\d+\s*$)",
    re.I
)

def split_tsv_line(raw: str) -> List[str]:
    return [c.strip() for c in raw.split("\t")]

def join_tsv_row(cells: List[str]) -> str:
    return "\t".join((c or "").strip() for c in cells)

def most_common_cols(lines: List[str]) -> int:
    counts = [len(split_tsv_line(ln)) for ln in lines if ln.strip()]
    if not counts: return 0
    return Counter(counts).most_common(1)[0][0]

def fix_common_table_row(raw: str) -> str:
    # s·ª≠a l·ªói OCR/ƒë√°nh m√°y nh·∫π
    raw = raw.replace("ÔÇ´", "‚òÖ")
    raw = re.sub(r"x·∫£y\s*xa\b", "x·∫£y ra", raw, flags=re.I)
    # g·ªôp CH·ªÆ IN B·ªä C√ÅCH K√ù T·ª∞: "K H ·∫¢ N ƒÇ N G" -> "KH·∫¢ NƒÇNG"
    raw = re.sub(r'((?:[A-Z√Ä-·ª∏ƒê]\s+){3,}[A-Z√Ä-·ª∏ƒê])', lambda m: m.group(0).replace(" ", ""), raw)
    return raw

def clean_table_block_strong(
    lines: List[str],
    min_cols: int,
    max_cols: int,
    fix_rows: bool = True,
    target_cols: int = 0
) -> List[str]:
    # 1) l·ªçc r√°c c∆° b·∫£n & t√°ch cells
    rows_cells: List[List[str]] = []
    filtered: List[str] = []
    for raw in lines:
        raw = fix_common_table_row(raw)
        if not raw.strip():           continue
        if NOISY_ROW_RE.match(raw):   continue
        filtered.append(raw)

    if not filtered:
        return []

    # 2) x√°c ƒë·ªãnh s·ªë c·ªôt chu·∫©n
    dom = target_cols if target_cols > 0 else most_common_cols(filtered)
    if dom == 0: dom = min_cols

    # 3) n·∫°p h√†ng, h·ª£p nh·∫•t h√†ng thi·∫øu c·ªôt v√†o √¥ cu·ªëi c·ªßa h√†ng tr∆∞·ªõc (wrap)
    for raw in filtered:
        cells = split_tsv_line(raw)
        if all(c == "" for c in cells):    continue
        if len(cells) < min_cols:          continue

        if fix_rows and rows_cells and 0 < len(cells) < dom:
            # n·ªëi v√†o cell cu·ªëi c·ªßa h√†ng tr∆∞·ªõc
            prev = rows_cells[-1]
            prev[-1] = (prev[-1] + " " + " ".join(cells)).strip()
            continue

        # chu·∫©n h√≥a s·ªë c·ªôt
        if len(cells) > max_cols:
            cells = cells[:max_cols]
        if dom and len(cells) < dom:
            cells = cells + [""] * (dom - len(cells))

        rows_cells.append([re.sub(r"\s+", " ", c) for c in cells])

    # 4) lo·∫°i header l·∫∑p l·∫°i (tr√πng y h·ªát c√°ch nhau ‚â•2 h√†ng)
    seen = set()
    cleaned_rows: List[List[str]] = []
    for i, row in enumerate(rows_cells):
        key = tuple(row)
        if key in seen:
            # b·ªè l·∫∑p r√µ r√†ng
            continue
        cleaned_rows.append(row)
        if i <= 5:  # ch·ªâ add v√†o seen cho v√†i h√†ng ƒë·∫ßu (th∆∞·ªùng l√† header)
            seen.add(key)

    # 5) xu·∫•t
    out_lines = []
    for r in cleaned_rows:
        if len(r) < min_cols:  # safety
            continue
        out_lines.append(join_tsv_row(r))
    return out_lines

# ===== Walk & process =====
def walk_text_files(in_root: Path, pattern: str) -> List[Path]:
    pats = [p.strip() for p in (pattern or "*_text.txt").split("|") if p.strip()]
    found = []
    for root, _, files in os.walk(in_root):
        for fn in files:
            for pat in pats:
                if Path(fn).match(pat):
                    found.append(Path(root) / fn); break
    return found

def process_one_file(
    input_txt: Path,
    in_root: Path,
    out_root: Path,
    skip_existing: bool,
    min_cols: int,
    max_cols: int,
    target_cols: int,
    no_merge_rows: bool
) -> Tuple[str, Path, Optional[Path]]:
    base_meta = load_base_meta(input_txt)

    rel_dir = input_txt.parent.relative_to(in_root)
    out_dir = out_root / rel_dir
    ensure_dir(out_dir)

    stem_in = input_txt.name.replace("_text.final.txt","").replace("_text.txt","")
    out_tsv = out_dir / f"{stem_in}_tables.final.tsv"
    out_jsonl = out_dir / f"{stem_in}_vector.jsonl"

    if skip_existing and out_tsv.exists() and out_jsonl.exists():
        return ("skip", out_tsv, out_jsonl)

    raw_lines = input_txt.read_text(encoding="utf-8", errors="ignore").splitlines()

    # gom ri√™ng c√°c block TABLE
    tables: List[Tuple[dict, List[str]]] = []
    cur_hdr = None
    cur_buf: List[str] = []

    def flush():
        nonlocal cur_hdr, cur_buf
        if cur_hdr and (cur_hdr.get("content_type_hint") or "").upper() == "TABLE":
            tables.append((cur_hdr, cur_buf[:]))
        cur_hdr = None; cur_buf = []

    for ln in raw_lines:
        if IS_HEADER.match(ln):
            # g·∫∑p header m·ªõi
            hdr = parse_header(ln)
            # x·∫£ block tr∆∞·ªõc
            flush()
            cur_hdr = hdr
        else:
            cur_buf.append(ln)
    flush()

    # l√†m s·∫°ch & ghi
    final_tsv_lines = []
    jsonl_records = []
    for hdr, buf in tables:
        lines = clean_table_block_strong(
            buf, min_cols=min_cols, max_cols=max_cols,
            fix_rows=(not no_merge_rows),
            target_cols=target_cols
        )
        if not lines:
            continue
        # ƒë√°nh d·∫•u block trong tsv t·ªïng
        final_tsv_lines.append(hdr.get("raw_header","### [TABLE]"))
        final_tsv_lines.extend(lines)

        # jsonl: m·ªói b·∫£ng l√† 1 record (g·ªôp to√†n b·ªô block)
        content = "\n".join(lines).strip()
        md = {k:v for k,v in hdr.items() if v is not None}
        for k,v in base_meta.items():
            if k == "text_sha1":  # b·ªè hash c≈©
                continue
            if k not in md:
                md[k] = v
        md["source_output"] = out_tsv.name
        jsonl_records.append({"content": content, "metadata": md, "content_type": "TABLE"})

    # ghi file
    out_tsv.write_text("\n".join(final_tsv_lines).strip() + ("\n" if final_tsv_lines else ""), encoding="utf-8")
    with open(out_jsonl, "w", encoding="utf-8") as f:
        for r in jsonl_records:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    return ("done", out_tsv, out_jsonl)

def main():
    ap = argparse.ArgumentParser(description="Clean TABLES only & export TSV + JSONL (mirror tree).")
    ap.add_argument("--in-root",  default=IN_ROOT_DEFAULT)
    ap.add_argument("--out-root", default=OUT_ROOT_DEFAULT)
    ap.add_argument("--pattern",  default=PATTERN_DEFAULT, help="*_text.txt|*_text.final.txt")
    ap.add_argument("--skip-existing", action="store_true", help="B·ªè qua file ƒë√£ c√≥ _tables.final.tsv v√† _vector.jsonl")
    ap.add_argument("--min-cols", type=int, default=1)
    ap.add_argument("--max-cols", type=int, default=1000)
    ap.add_argument("--target-cols", type=int, default=0, help="N·∫øu >0, √©p s·ªë c·ªôt chu·∫©n cho b·∫£ng")
    ap.add_argument("--no-merge-rows", action="store_true", help="Kh√¥ng t·ª± n·ªëi h√†ng thi·∫øu c·ªôt v√†o h√†ng tr∆∞·ªõc")
    # ch·∫•p nh·∫≠n --start/--end cho quen tay (b·ªè qua)
    ap.add_argument("--start", type=int, default=None)
    ap.add_argument("--end",   type=int, default=None)
    args = ap.parse_args()
    p.add_argument(
        "--no-merge-rows",
        dest="no_merge_rows",
        action="store_true",
        help="Kh√¥ng g·ªôp 2 d√≤ng header ƒë·∫ßu v√†o 1 (gi·ªØ nguy√™n t·ª´ng d√≤ng)."
    )

    in_root = Path(args.in_root).resolve()
    out_root = Path(args.out_root).resolve()
    if not in_root.exists():
        print(f"‚ö†Ô∏è in-root kh√¥ng t·ªìn t·∫°i: {in_root}"); return
    ensure_dir(out_root)

    files = walk_text_files(in_root, args.pattern)
    if not files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file theo pattern"); return

    print(f"üß≠ {len(files)} file ‚Äî TABLES only\nüìÇ Input : {in_root}\nüì¶ Output: {out_root}")
    done = skipped = 0
    for i, p in enumerate(sorted(files), 1):

        status, p_tsv, p_jsonl = process_one_file(
            input_txt=p, in_root=in_root, out_root=out_root,
            skip_existing=args.skip_existing,
            min_cols=args.min_cols, max_cols=args.max_cols,
            target_cols=args.target_cols,
            no_merge_rows=getattr(args, "no_merge_rows", False),
        )

        if status == "skip":
            skipped += 1; print(f"[{i}/{len(files)}] ‚è≠Ô∏è  {p.relative_to(in_root)} (ƒë√£ c√≥)")
        else:
            done += 1;   print(f"[{i}/{len(files)}] ‚úÖ {p.relative_to(in_root)} -> {p_tsv.relative_to(out_root)} ; JSONL -> {p_jsonl.relative_to(out_root)}")
    print(f"üéØ Ho√†n t·∫•t. X·ª≠ l√Ω: {done}, b·ªè qua: {skipped}, t·ªïng: {len(files)}")

if __name__ == "__main__":
    main()
