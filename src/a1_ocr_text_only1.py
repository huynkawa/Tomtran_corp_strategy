# -*- coding: utf-8 -*-
"""
a1_ocr_text_only.py ‚Äî OCR v√† ƒë·ªçc vƒÉn b·∫£n ƒëa ƒë·ªãnh d·∫°ng (PDF, DOCX, EXCEL, CSV, IMAGE)

M·ª•c ti√™u:
---------
- ƒê·ªçc t·∫•t c·∫£ c√°c lo·∫°i t√†i li·ªáu (PDF, DOCX, XLSX, CSV, IMAGE, TXT)
- Kh√¥ng sinh ra b·∫•t k·ª≥ file CSV/Excel n√†o.
- Chuy·ªÉn to√†n b·ªô n·ªôi dung (k·ªÉ c·∫£ b·∫£ng scan, s∆° ƒë·ªì) sang text duy nh·∫•t.
- Xu·∫•t ƒë√∫ng 2 file/trang:
    <base>_page{n}_text.txt
    <base>_page{n}_meta.json
- Gi·ªØ c·∫•u tr√∫c th∆∞ m·ª•c mirror t·ª´ inputs sang outputs.
- Khi OCR ·∫£nh ho·∫∑c PDF scan ‚Üí d√πng TSV reflow ƒë·ªÉ t√°i c·∫•u tr√∫c d√≤ng/b·∫£ng.
- B·ªï sung metadata m·ªü r·ªông t·ª´ YAML rule v√† ch√®n heading anchors h·ªó tr·ª£ chunking RAG.

ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh:
-------------------
Input : D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\inputs\a_text_only_inputs_test
Output: D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\a_text_only_outputs
Rules : D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\configs\a1_text_only_rules.yaml

Y√™u c·∫ßu th∆∞ vi·ªán:
-----------------
pip install pdf2image pillow opencv-python-headless numpy pytesseract python-docx pandas openpyxl tqdm pyyaml
v√† c√†i ƒë·∫∑t Tesseract (tesseract.exe c√≥ trong PATH ho·∫∑c ƒë·∫∑t env TESSERACT_CMD)
"""

from __future__ import annotations
import os, re, glob, json, argparse, hashlib, shutil, time
from typing import Optional, Tuple, Dict, List
from pathlib import Path

import numpy as np
import cv2
from PIL import Image
import pytesseract
from pdf2image import convert_from_path
from pytesseract import Output as TessOutput
import pandas as pd
from tqdm import tqdm

# YAML loader (∆∞u ti√™n PyYAML, fallback ruamel)
try:
    import yaml  # type: ignore
except Exception:
    yaml = None
    try:
        from ruamel import yaml as ruamel_yaml  # type: ignore
    except Exception:
        ruamel_yaml = None

try:
    import docx
except ImportError:
    docx = None

# =========================
# ‚öôÔ∏è C·∫§U H√åNH C∆† B·∫¢N
# =========================
INPUT_DIR_DEFAULT = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\inputs\a_text_only_inputs_test"
OUTPUT_DIR_DEFAULT = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\a_text_only_outputs1"
RULES_PATH_DEFAULT = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\configs\a1_text_only_rules.yaml"

OCR_LANG_DEFAULT = "vie+eng"
OCR_CFG_DEFAULT  = "--psm 6 preserve_interword_spaces=1"
APPEND_MODE = False

TESSERACT_CMD = os.environ.get("TESSERACT_CMD", None)
if TESSERACT_CMD and os.path.isfile(TESSERACT_CMD):
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_CMD

# =========================
# ‚öôÔ∏è H·ªñ TR·ª¢
# =========================
def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)

def clean_txt_chars(s: str) -> str:
    """Chu·∫©n ho√° vƒÉn b·∫£n OCR: lo·∫°i bullet r√°c & kho·∫£ng tr·∫Øng th·ª´a."""
    if not s: return ""
    s = re.sub(r"[|¬¶‚Ä¢ÔÇ∑ÔÇü‚àô¬∑‚ñ†‚ñ°‚ñ™‚ó¶‚óè‚óã‚óª‚óº‚ñ∂‚ñ∫‚Ä¢‚óè‚óÜ‚óá‚òÖ‚òÜ‚ñ†‚ñ°-]{2,}", " ", s)
    s = re.sub(r"\xa0", " ", s)             # non-breaking space
    s = re.sub(r"[^\S\r\n]{2,}", " ", s)    # nhi·ªÅu space -> 1
    s = re.sub(r"[^\x09\x0A\x0D\x20-\x7E\u00A0-\uFFFF]", " ", s)
    # fix d√≠nh ch·ªØ th∆∞·ªùng g·∫∑p khi OCR t·ª´ DOCX/PDF
    s = re.sub(r"(?<=\d)(?=[A-Za-z])", " ", s)
    s = re.sub(r"(?<=[a-z])(?=\d)", " ", s)
    return s.strip()

def _sha1_text(s: str) -> str:
    return hashlib.sha1((s or "").encode("utf-8")).hexdigest()

def _sha1_file(path: str) -> str:
    h = hashlib.sha1()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()

def detect_language(text: str) -> str:
    if not text: return "vi"
    vi_marks = re.findall(r"[ƒÉ√¢√™√¥∆°∆∞ƒë√°√†·∫£√£·∫°√©√®·∫ª·∫Ω·∫π√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√∫√π·ªß≈©·ª•√Ω·ª≥·ª∑·ªπ·ªµ]", text.lower())
    if len(vi_marks) >= 3: return "vi"
    if re.search(r"\b(January|February|March|April|May|June|July|August|September|October|November|December)\b", text, re.IGNORECASE):
        return "en"
    return "vi"

# =========================
# ‚öôÔ∏è ƒê·ªåC YAML RULES
# =========================
def load_rules(path: str) -> dict:
    if not path or not os.path.exists(path):
        return {}
    try:
        if yaml:
            with open(path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        elif ruamel_yaml:
            with open(path, "r", encoding="utf-8") as f:
                return ruamel_yaml.YAML(typ="safe").load(f) or {}
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c YAML rules: {e}")
    return {}

def match_rules(file_path: str, text: str, rules: dict) -> dict:
    """
    T√¨m rule kh·ªõp theo t√™n file (∆∞u ti√™n) ho·∫∑c pattern trong text.
    YAML format g·ª£i √Ω:
    ---
    files:
      - match: "(?i)Appen6.*Retention"
        company: "UIC"
        doc_title: "APPENDIX 6 - RETENTION GUIDELINE"
        doc_type: "Underwriting Guideline / Appendix"
        appendix_id: "Appendix 6"
    defaults:
      department: "UWRI"
      confidentiality: "Internal"
    """
    if not rules: return {}
    res = {}
    defaults = rules.get("defaults", {}) or {}
    files = rules.get("files", []) or []
    fname = os.path.basename(file_path)
    for item in files:
        pat = item.get("match")
        if not pat: continue
        try:
            if re.search(pat, fname, flags=re.IGNORECASE):
                res = {**defaults, **{k:v for k,v in item.items() if k!="match"}}
                break
        except re.error:
            continue

    # N·∫øu ch∆∞a kh·ªõp theo t√™n file, th·ª≠ kh·ªõp theo text
    if not res and files:
        for item in files:
            txt_pat = item.get("text_match")
            if not txt_pat: continue
            try:
                if re.search(txt_pat, text or "", flags=re.IGNORECASE):
                    res = {**defaults, **{k:v for k,v in item.items() if k not in ("match","text_match")}}
                    break
            except re.error:
                continue
    # B·ªìi defaults n·∫øu c√≥
    if defaults and res:
        res = {**defaults, **res}
    return res

# =========================
# ‚öôÔ∏è TSV REFLOW cho OCR b·∫£ng/scan
# =========================
def reflow_lines_from_tsv_dict(data: Dict[str, List], y_tol: int = 4) -> str:
    n = len(data.get("text", []))
    groups: Dict[Tuple[int,int,int], List[int]] = {}
    for i in range(n):
        t = (data["text"][i] or "").strip()
        if not t: continue
        key = (int(data["block_num"][i]), int(data["par_num"][i]), int(data["line_num"][i]))
        groups.setdefault(key, []).append(i)

    lines = []
    for key, idxs in sorted(groups.items()):
        idxs = sorted(idxs, key=lambda k: int(data["left"][k]))
        txt = " ".join((data["text"][k] or "").strip() for k in idxs).strip()
        if not txt: continue
        y = int(min(int(data["top"][k]) for k in idxs))
        lines.append({"y": y, "text": txt})

    lines.sort(key=lambda r: r["y"])
    out_lines = []
    for ln in lines:
        s = ln["text"]
        # √©p xu·ªëng d√≤ng tr∆∞·ªõc c√°c m√£ s·ªë, m·ª•c l·ªõn, s·ªë ti·ªÅn
        s = re.sub(r"(?<!^)\s(?=\d{3}(?:\.\d+)?\b)", "\n", s)
        s = re.sub(r"(?<!^)\s(?=(?:I|II|III|IV|V|VI|VII|VIII|IX|X)\.?\b)", "\n", s)
        s = re.sub(r"(?<!^)\s(?=\d{1,3}(?:[.,]\d{3}){2,}\b)", "\n", s)
        out_lines.extend([p.strip() for p in s.split("\n") if p.strip()])
    return "\n".join(out_lines)

def ocr_image_to_text_tsv(img_bgr, ocr_lang: str, ocr_cfg: str) -> str:
    try:
        cfg = (ocr_cfg or "").strip()
        cfg = re.sub(r"--psm\s+\d+", "", cfg)
        cfg = (cfg + " --psm 4 preserve_interword_spaces=1").strip()
        tsv = pytesseract.image_to_data(img_bgr, lang=ocr_lang, config=cfg, output_type=TessOutput.DICT)
        txt = reflow_lines_from_tsv_dict(tsv)
        return clean_txt_chars(txt)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói OCR TSV: {e}")
        return ""

# =========================
# ‚öôÔ∏è ƒê·ªåC ·∫¢NH & PDF SCAN
# =========================
def pdf_to_texts(pdf_path: str, dpi: int = 400,
                 ocr_lang: str = OCR_LANG_DEFAULT, ocr_cfg: str = OCR_CFG_DEFAULT,
                 start_page: Optional[int] = None, end_page: Optional[int] = None) -> List[Tuple[int, str]]:
    texts = []
    try:
        pages = convert_from_path(pdf_path, dpi=dpi)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói m·ªü PDF {pdf_path}: {e}")
        return texts

    total = len(pages)
    s = start_page or 1
    e = end_page or total
    s = max(1, s); e = min(total, e)
    for i in range(s, e+1):
        page = pages[i-1]
        img = cv2.cvtColor(np.array(page), cv2.COLOR_RGB2BGR)
        txt = ocr_image_to_text_tsv(img, ocr_lang, ocr_cfg)
        texts.append((i, txt))
    return texts

# =========================
# ‚öôÔ∏è ƒê·ªåC FILE TEXT/DOCX/EXCEL/CSV
# =========================
def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def read_docx_file(path: str) -> str:
    if docx is None:
        raise ImportError("‚ö†Ô∏è C·∫ßn c√†i python-docx ƒë·ªÉ ƒë·ªçc DOCX.")
    d = docx.Document(path)
    # L·∫•y paragraph + table (n·∫øu c√≥) theo d·∫°ng m√¥ t·∫£
    parts = []
    for p in d.paragraphs:
        if p.text.strip():
            parts.append(p.text)
    # B·∫£ng: n·ªëi theo h√†ng
    for tbl in d.tables:
        parts.append("========== TABLE ==========")
        for r in tbl.rows:
            cells = [clean_txt_chars(c.text) for c in r.cells]
            if any(c.strip() for c in cells):
                parts.append(" | ".join(cells))
    return "\n".join(parts)

def read_excel_or_csv(path: str, mode: str = "summary") -> str:
    ext = Path(path).suffix.lower()
    texts = []
    try:
        if ext == ".csv":
            df = pd.read_csv(path, dtype=str)
            dfs = {"CSV": df}
        else:
            xls = pd.ExcelFile(path, engine="openpyxl")
            dfs = {sheet: pd.read_excel(xls, sheet_name=sheet, dtype=str)
                   for sheet in xls.sheet_names}

        for sheet_name, df in dfs.items():
            if df.empty:
                continue
            df = df.fillna("").astype(str)
            if mode == "raw":
                headers = list(df.columns)
                header_line = " | ".join(headers)
                rows = [" | ".join(row.values) for _, row in df.iterrows()]
                sheet_text = f"\n\n========== SHEET: {sheet_name.upper()} ==========\n{header_line}\n" + "\n".join(rows)
            else:
                df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
                headers = list(df.columns)
                sheet_text = [f"\n========== SHEET: {sheet_name.upper()} =========="]
                sheet_text.append(f"C√°c c·ªôt g·ªìm: {', '.join(headers)}.")
                for _, row in df.iterrows():
                    if not any(v.strip() for v in row.values):
                        continue
                    pairs = [f"{h}: {v}" for h, v in row.items() if v.strip()]
                    sheet_text.append("; ".join(pairs))
                sheet_text = "\n".join(sheet_text)
            texts.append(sheet_text.strip())

        return "\n\n".join(texts).strip()
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc {path}: {e}")
        return ""

# =========================
# ‚öôÔ∏è POSTPROCESS: Anchors & chu·∫©n ho√°
# =========================
RE_CHAPTER = re.compile(r"^\s*(CHAPTER|CHAP\.?)\s+([IVXLC]+)\b(.*)$", re.IGNORECASE)
RE_SECTION = re.compile(r"^\s*(SECTION|SEC\.?)\s+([A-Z0-9]+)\b(.*)$", re.IGNORECASE)
RE_PART    = re.compile(r"^\s*(PART|PT\.?)\s+([A-Z0-9]+)\b(.*)$", re.IGNORECASE)

def add_heading_anchors(text: str) -> Tuple[str, List[str]]:
    """
    Chuy·ªÉn c√°c heading sang Markdown anchors:
      CHAPTER I ...  -> # CHAPTER I ...
      SECTION A ...  -> ## SECTION A ...
      PART 1 ...     -> ### PART 1 ...
    Tr·∫£ v·ªÅ (text_m·ªõi, danh_s√°ch_anchors)
    """
    anchors: List[str] = []
    out_lines = []
    for line in (text or "").splitlines():
        raw = line.strip()
        if not raw:
            out_lines.append("")
            continue

        m = RE_CHAPTER.match(raw)
        if m:
            head = f"CHAPTER {m.group(2)}{m.group(3)}".strip()
            anchors.append(head)
            out_lines.append(f"# {head}")
            continue

        m = RE_SECTION.match(raw)
        if m:
            head = f"SECTION {m.group(2)}{m.group(3)}".strip()
            anchors.append(head)
            out_lines.append(f"## {head}")
            continue

        m = RE_PART.match(raw)
        if m:
            head = f"PART {m.group(2)}{m.group(3)}".strip()
            anchors.append(head)
            out_lines.append(f"### {head}")
            continue

        # Chu·∫©n ho√° bullet/dash ƒë∆°n gi·∫£n
        line2 = re.sub(r"^\s*[-‚Ä¢¬∑]\s*", "- ", raw)
        out_lines.append(line2)

    txt2 = "\n".join(out_lines)
    # Gom b·ªõt d√≤ng tr·ªëng, lo·∫°i b·ªôi kho·∫£ng tr·∫Øng
    txt2 = re.sub(r"\n{3,}", "\n\n", txt2).strip()
    return txt2, sorted(list(dict.fromkeys(anchors)))  # gi·ªØ th·ª© t·ª± xu·∫•t hi·ªán

def apply_rules_and_enrich_meta(
    file_path: str, base: str, text: str, meta: dict, rules: dict
) -> Tuple[str, dict]:
    """
    - √Åp YAML rules ƒë·ªÉ ƒëi·ªÅn company/doc_title/doc_type/...
    - Ch√®n anchors v√†o text, ƒë∆∞a danh s√°ch anchors v√†o meta.
    """
    # 1) Anchors
    text_clean = clean_txt_chars(text)
    text_anchored, anchors = add_heading_anchors(text_clean)

    # 2) Rules ‚Üí extended meta
    matched = match_rules(file_path, text_anchored, rules)
    if matched:
        meta.update({
            "company":        matched.get("company"),
            "doc_title":      matched.get("doc_title"),
            "doc_type":       matched.get("doc_type"),
            "department":     matched.get("department"),
            "lob":            matched.get("lob"),
            "appendix_id":    matched.get("appendix_id"),
            "effective_date": matched.get("effective_date"),
            "version":        matched.get("version"),
            "confidentiality":matched.get("confidentiality")
        })

    # 3) Anchors ‚Üí meta
    if anchors:
        meta["anchors"] = anchors

    return text_anchored, meta

# =========================
# ‚öôÔ∏è GHI OUTPUT (t√¥n tr·ªçng APPEND_MODE)
# =========================
def save_output_text_and_meta(text: str, meta: dict, out_txt: str, out_meta: str):
    ensure_dir(os.path.dirname(out_txt))

    # N·∫øu APPEND_MODE v√† file ƒë√£ t·ªìn t·∫°i, b·ªè qua n·∫øu sha1 kh√¥ng ƒë·ªïi
    if APPEND_MODE and os.path.exists(out_txt) and os.path.exists(out_meta):
        try:
            with open(out_txt, "r", encoding="utf-8") as f:
                old = f.read()
            old_sha = _sha1_text(old)
            new_sha = _sha1_text(text)
            if old_sha == new_sha:
                print(f"‚è≠Ô∏è B·ªè qua (kh√¥ng ƒë·ªïi): {os.path.basename(out_txt)}")
                return
        except Exception:
            pass

    with open(out_txt, "w", encoding="utf-8") as f:
        f.write(text)
    with open(out_meta, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print(f"üìù Saved: {os.path.basename(out_txt)}, {os.path.basename(out_meta)}")

# =========================
# ‚öôÔ∏è X·ª¨ L√ù FILE CH√çNH
# =========================
def process_file(file_path: str, input_root: str, output_root: str,
                 ocr_lang: str, ocr_cfg: str, rules: dict,
                 dpi: int = 400,
                 start_page: Optional[int] = None, end_page: Optional[int] = None,
                 excel_mode: str = "summary"):

    rel_path = os.path.relpath(file_path, input_root)
    base = Path(file_path).stem
    out_dir = os.path.join(output_root, os.path.dirname(rel_path))
    ensure_dir(out_dir)

    ext = Path(file_path).suffix.lower()
    text_outputs: List[Tuple[int, str]] = []

    if ext in [".jpg", ".jpeg", ".png", ".tif", ".tiff", ".bmp"]:
        img = cv2.cvtColor(np.array(Image.open(file_path).convert("RGB")), cv2.COLOR_RGB2BGR)
        txt = ocr_image_to_text_tsv(img, ocr_lang, ocr_cfg)
        text_outputs = [(1, txt)]

    elif ext == ".pdf":
        print(f"üìÑ {os.path.basename(file_path)} ‚Üí OCR (PDF)")
        t0 = time.perf_counter()
        text_outputs = pdf_to_texts(file_path, dpi=dpi, ocr_lang=ocr_lang, ocr_cfg=ocr_cfg,
                                    start_page=start_page, end_page=end_page)
        t1 = time.perf_counter()
        print(f"üïì OCR xong {len(text_outputs)} trang ({t1 - t0:.1f}s)")

    elif ext in [".doc", ".docx"]:
        try:
            txt = read_docx_file(file_path)
            text_outputs = [(1, txt)]
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói DOCX: {e}")

    elif ext in [".xls", ".xlsx", ".csv"]:
        txt = read_excel_or_csv(file_path, mode=excel_mode)
        text_outputs = [(1, txt)]

    elif ext == ".txt":
        txt = read_text_file(file_path)
        text_outputs = [(1, txt)]

    else:
        print(f"‚ö†Ô∏è B·ªè qua (ƒë·ªãnh d·∫°ng kh√¥ng h·ªó tr·ª£): {file_path}")
        return

    # Ghi k·∫øt qu·∫£
    src_sha1 = _sha1_file(file_path)
    if len(text_outputs) > 1:
        combined_text = "\n\n".join(t for _, t in text_outputs if t.strip())
        page_range = f"{start_page or 1}-{end_page or (start_page or len(text_outputs))}"
        out_txt = os.path.join(out_dir, f"{base}_page{page_range}_text.txt")
        out_meta = os.path.join(out_dir, f"{base}_page{page_range}_meta.json")
        meta = {
            "file": base,
            "page_range": page_range,
            "page_count": len(text_outputs),
            "source_path": os.path.abspath(file_path),
            "source_doc_sha1": src_sha1,
            "language": detect_language(combined_text),
            "ocr_lang": ocr_lang,
            "ocr_cfg": ocr_cfg,
            "text_sha1": _sha1_text(combined_text),
        }
        combined_text, meta = apply_rules_and_enrich_meta(file_path, base, combined_text, meta, rules)
        save_output_text_and_meta(combined_text, meta, out_txt, out_meta)
    else:
        for page_no, txt in text_outputs:
            out_txt = os.path.join(out_dir, f"{base}_page{page_no}_text.txt")
            out_meta = os.path.join(out_dir, f"{base}_page{page_no}_meta.json")
            meta = {
                "file": base,
                "page": page_no,
                "source_path": os.path.abspath(file_path),
                "source_doc_sha1": src_sha1,
                "language": detect_language(txt),
                "ocr_lang": ocr_lang,
                "ocr_cfg": ocr_cfg,
                "text_sha1": _sha1_text(txt),
            }
            txt2, meta2 = apply_rules_and_enrich_meta(file_path, base, txt, meta, rules)
            save_output_text_and_meta(txt2, meta2, out_txt, out_meta)

# =========================
# ‚öôÔ∏è MAIN ENTRYPOINT
# =========================
def main():
    parser = argparse.ArgumentParser("A1 ‚Äî OCR ƒëa ƒë·ªãnh d·∫°ng (Text only, TSV reflow + YAML rules + anchors)")
    parser.add_argument("--input", type=str, default=INPUT_DIR_DEFAULT, help="Th∆∞ m·ª•c input")
    parser.add_argument("--out", type=str, default=OUTPUT_DIR_DEFAULT, help="Th∆∞ m·ª•c output")
    parser.add_argument("--rules", type=str, default=RULES_PATH_DEFAULT, help="ƒê∆∞·ªùng d·∫´n YAML rules")
    parser.add_argument("--ocr-lang", type=str, default=OCR_LANG_DEFAULT)
    parser.add_argument("--ocr-cfg", type=str, default=OCR_CFG_DEFAULT)
    parser.add_argument("--dpi", type=int, default=400)
    parser.add_argument("--clean", choices=["y","a","n","ask"], default="ask")
    parser.add_argument("--start", type=int, default=None, help="Trang b·∫Øt ƒë·∫ßu (PDF)")
    parser.add_argument("--end", type=int, default=None, help="Trang k·∫øt th√∫c (PDF)")
    parser.add_argument("--excel-mode", choices=["raw", "summary"], default="summary",
                        help="C√°ch ƒë·ªçc Excel: raw=gi·ªØ nguy√™n, summary=l√†m s·∫°ch ƒë·ªÉ vector store")

    args = parser.parse_args()
    START_PAGE, END_PAGE = args.start, args.end

    # X·ª≠ l√Ω th∆∞ m·ª•c output
    if os.path.exists(args.out):
        choice = args.clean
        if choice == "ask":
            try:
                choice = input(f"‚ö†Ô∏è Output '{args.out}' ƒë√£ t·ªìn t·∫°i. y=xo√°, a=append, n=b·ªè qua: ").strip().lower()
            except Exception:
                choice = "a"  # n·∫øu ch·∫°y kh√¥ng c√≥ stdin
        if choice == "y":
            shutil.rmtree(args.out, ignore_errors=True)
            print(f"üóëÔ∏è ƒê√£ xo√° {args.out}")
        elif choice == "a":
            global APPEND_MODE
            APPEND_MODE = True
            print(f"‚ûï Gi·ªØ {args.out}, ch·ªâ ghi file m·ªõi/kh√°c n·ªôi dung.")
        elif choice == "n":
            print("‚è≠Ô∏è B·ªè qua to√†n b·ªô."); return
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá."); return

    ensure_dir(args.out)
    files = glob.glob(os.path.join(args.input, "**", "*.*"), recursive=True)
    if not files:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file n√†o trong input."); return

    # N·∫°p YAML rules
    rules = load_rules(args.rules)
    if rules:
        print(f"üß© ƒê√£ n·∫°p rules: {args.rules}")
    else:
        print(f"‚ÑπÔ∏è Kh√¥ng t√¨m th·∫•y/kh√¥ng ƒë·ªçc ƒë∆∞·ª£c rules: {args.rules} (v·∫´n ch·∫°y b√¨nh th∆∞·ªùng)")

    print(f"üìÇ Input : {args.input}")
    print(f"üì¶ Output: {args.out}")
    print(f"üßÆ T·ªïng s·ªë file: {len(files)}")
    print(f"üß≠ Gi·ªõi h·∫°n trang PDF: {START_PAGE or 1} ‚Üí {END_PAGE or 't·∫•t c·∫£'}")
    has_excel = any(f.lower().endswith((".xls", ".xlsx", ".csv")) for f in files)
    if has_excel:
        print(f"üìä Ch·∫ø ƒë·ªô ƒë·ªçc Excel: {args.excel_mode}")

    for f in files:
        process_file(f, args.input, args.out,
                     args.ocr_lang, args.ocr_cfg, rules,
                     dpi=args.dpi, start_page=START_PAGE, end_page=END_PAGE,
                     excel_mode=args.excel_mode)

    print("\n‚úÖ Ho√†n t·∫•t OCR. Ki·ªÉm tra *_text.txt v√† *_meta.json trong th∆∞ m·ª•c output.")

if __name__ == "__main__":
    main()
