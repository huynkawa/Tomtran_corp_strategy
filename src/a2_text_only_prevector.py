# -*- coding: utf-8 -*-
"""
src.a2_clean_text_only_prevector.py
‚Äî Final CLEAN (TEXT ONLY) tr∆∞·ªõc khi index v√†o vector store
‚Äî ƒê·ªçc t·ª´ outputs\\a1_text_only_outputs ‚Üí ghi sang outputs\\a2_text_only_prevector (mirror tree)

Thay ƒë·ªïi ch√≠nh (so v·ªõi b·∫£n tr∆∞·ªõc):
- CH·ªà x·ª≠ l√Ω TEXT (b·ªè to√†n b·ªô TABLE/TSV).
- N√¢ng c·∫•p l√†m s·∫°ch: gi·ªØ/chu·∫©n ho√° ti·ªÅn t·ªá ($, ‚Ç´/ƒë, VND/VNƒê), chu·∫©n ho√° d·∫•u c√¢u cong ‚Üí ASCII,
  n·ªëi d√≤ng ‚Äúm·ªÅm‚Äù kh√¥ng l√†m d√≠nh t·ª´, l·ªçc watermark/ads (Bookey, ‚ÄúSSccaann ttoo DDoowwnnllooaadd‚Äù, ‚Ä¶).
- H·ªèi 1 l·∫ßn ch·∫ø ƒë·ªô ghi:
    Y = purge  (x√≥a 3 file c≈© r·ªìi t·∫°o l·∫°i m·ªõi)
    N = skip   (n·∫øu ƒë√£ c√≥ ƒë·ªß file ƒë√≠ch th√¨ b·ªè qua)
    A = append (vector.jsonl: append; text/meta: ghi l·∫°i ƒë·ªÉ ƒë·ªìng b·ªô)
- L·ªçc theo trang --start/--end d·ª±a tr√™n marker A1: "### [PDF page X] [TEXT]"

V√≠ d·ª•:
    python -m src.a2_clean_text_only_prevector --start 26 --end 28
    python -m src.a2_clean_text_only_prevector

Ghi ch√∫:
- B∆∞·ªõc n√†y kh√¥ng c·∫ßn YAML (rule-based). C√≥ th·ªÉ b·ªï sung YAML sau (patterns, replace-map) n·∫øu mu·ªën.
"""

import os, re, json, argparse, hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple

# ============== ƒê∆Ø·ªúNG D·∫™N M·∫∂C ƒê·ªäNH ==============
IN_ROOT_DEFAULT  = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\a1_text_only_outputs"
OUT_ROOT_DEFAULT = r"D:\1.TLAT\3. ChatBot_project\1_Insurance_Strategy\outputs\a2_text_only_prevector"

# ============== H·ªéI 1 L·∫¶N CH·∫æ ƒê·ªò GHI ==============
def ask_write_mode_once() -> str:
    """
    Y = purge   (x√≥a file c≈© r·ªìi t·∫°o m·ªõi)
    N = skip    (n·∫øu ƒë√£ c√≥ ƒë·ªß file ƒë√≠ch -> b·ªè qua)
    A = append  (text/meta: ghi l·∫°i; vector.jsonl: append)
    """
    print("Ch·∫ø ƒë·ªô ghi to√†n b·ªô l∆∞·ª£t ch·∫°y (Y=purge, N=skip, A=append):")
    while True:
        a = input("Ch·ªçn [Y/N/A]: ").strip().upper()
        if a in ("Y","N","A"):
            return {"Y":"purge","N":"skip","A":"append"}[a]
        print("Vui l√≤ng nh·∫≠p Y / N / A.")

# ============== TI·ªÜN √çCH C∆† B·∫¢N ==============
def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def sha1_of_text(text: str) -> str:
    return hashlib.sha1((text or "").encode("utf-8")).hexdigest()

def find_pairs(in_root: Path) -> List[Tuple[Path, Path]]:
    """
    T√¨m t·∫•t c·∫£ c·∫∑p (text_path, meta_path) theo pattern *_text.txt / *_meta.json
    trong c√¢y th∆∞ m·ª•c IN_ROOT.
    """
    pairs = []
    for txt_path in in_root.rglob("*_text.txt"):
        meta_path = txt_path.with_name(txt_path.name.replace("_text.txt", "_meta.json"))
        if meta_path.exists():
            pairs.append((txt_path, meta_path))
    return pairs

# ============== CLEAN TEXT (RULE-BASED) ==============
# D√≤ng r√°c/header/footer ph·ªï bi·∫øn
TEXT_BADLINES_PATTERNS = [
    r"^\s*page\s*\d+\s*/\s*\d+\s*$",          # "Page 3/20"
    r"^\s*\d+\s*$",                           # d√≤ng ch·ªâ c√≥ s·ªë (th∆∞·ªùng l√† s·ªë trang)
    r"^\s*copyright.*$",
    r"^\s*figure\s*\d+.*$",
    r"^\s*table\s*\d+.*$",
    r"^More\s+Free\s+Books\s+on\s+Bookey.*$", # qu·∫£ng c√°o
    r"SSccaann\s*ttoo\s*DDoowwnnllooaadd",    # watermark m√©o OCR
    r"^\s*Huy\s*$",                           # r√°c ƒë∆°n l·∫ª (ƒë·∫∑t theo ph√°t hi·ªán th·ª±c t·∫ø)
]

# Map d·∫•u c√¢u "cong" ‚Üí ASCII/Unicode an to√†n cho vector
PUNCT_MAP = {
    "\u2018": "'",   # ‚Äò ‚Üí '
    "\u2019": "'",   # ‚Äô ‚Üí '
    "\u201C": '"',   # ‚Äú ‚Üí "
    "\u201D": '"',   # ‚Äù ‚Üí "
    "\u2013": "-",   # ‚Äì (en-dash) ‚Üí hyphen
    "\u2014": " ‚Äî ", # ‚Äî (em-dash) ‚Üí em-dash c√≥ kho·∫£ng tr·∫Øng 2 b√™n
}

# Cho ph√©p (ASCII + ti·∫øng Vi·ªát + d·∫•u c√¢u c∆° b·∫£n + EM DASH '‚Äî')
WEIRD_CHARS = r"[^A-Za-z0-9√Ä-·ªπ.,:;?!%/\-\(\)\[\]{}_&'\" \t‚Äî]"

def _apply_punct_map(text: str) -> str:
    # √Ånh x·∫° d·∫•u c√¢u cong ‚Üí th·∫≥ng
    for k, v in PUNCT_MAP.items():
        text = text.replace(k, v)
    # CH·ªà chu·∫©n ho√° kho·∫£ng tr·∫Øng quanh em-dash ‚Äî (kh√¥ng ƒë·ª•ng v√†o '-')
    text = re.sub(r"\s*‚Äî\s*", " ‚Äî ", text)
    return text

def _normalize_ascii_dash(text: str) -> str:
    """
    ƒê·ªïi ' - ' th√†nh em-dash ' ‚Äî ' khi ·ªü gi·ªØa 2 token d√†i (‚â•2 k√Ω t·ª±).
    Kh√¥ng ph√° 'A-B', 'Pro-X', v.v.
    """
    # C√°ch 1 (ƒë∆°n gi·∫£n, Unicode \w): tr√°i ‚â•2 k√Ω t·ª±, ph·∫£i ‚â•2 k√Ω t·ª± (d√πng lookahead c·ªë ƒë·ªãnh)
    text = re.sub(r'(\b\w{2,})\s-\s(?=\w{2,}\b)', r'\1 ‚Äî ', text)
    # C√°ch 2 (b·ªï sung cho t·ª´ c√≥ d·∫•u nh√°y ti·∫øng Vi·ªát/Anh nh∆∞ "man's", ‚ÄúnƒÉm‚Äôs‚Äù)
    text = re.sub(r"([A-Za-z√Ä-·ªπ0-9][A-Za-z√Ä-·ªπ0-9'‚Äô]{1,})\s-\s(?=[A-Za-z√Ä-·ªπ0-9'‚Äô]{2,}\b)", r"\1 ‚Äî ", text)
    return text


def _soft_join_hyphen(text: str) -> str:
    """
    N·ªëi d√≤ng khi cu·ªëi d√≤ng c√≥ '-' (hyphen):
    - V·∫ø tr√°i NG·∫ÆN (‚â§3 k√Ω t·ª±)  ‚Üí GI·ªÆ hyphen: 'no-\nman's' -> 'no-man's'
    - V·∫ø tr√°i D√ÄI  (‚â•4 k√Ω t·ª±)  ‚Üí B·ªé hyphen: 'inves-\n tment' -> 'investment'
    (Kh√¥ng d√πng look-behind ƒë·ªÉ tr√°nh l·ªói variable-width)
    """
    # Gi·ªØ hyphen cho ti·ªÅn t·ªë ng·∫Øn
    text = re.sub(r'(\b\w{1,3})-\s*\n(?=\w)', r'\1-', text)
    # B·ªè hyphen cho v·∫ø tr√°i d√†i
    text = re.sub(r'(\b\w{4,})-\s*\n(?=\w)', r'\1', text)
    return text



def _soft_join_sentences(text: str) -> str:
    # n·ªëi d√≤ng m·ªÅm: n·∫øu gi·ªØa hai d√≤ng kh√¥ng k·∫øt th√∫c b·∫±ng .!?;: th√¨ n·ªëi b·∫±ng 1 space
    return re.sub(r"(?<![\.!?;:])\n(?!\n)", " ", text)


# --- Currency normalizer (ƒë·∫∑t TR√äN clean_text_block) ---
CURRENCY_PAT_DOLLAR = re.compile(
    r"(?<![A-Za-z])\$(\d{1,3}(?:,\d{3})*(?:\.\d+)?|\d+(?:\.\d+)?)"
)
CURRENCY_PAT_VND = re.compile(
    r"\b(?:VNƒê|VND|vnƒë|vnd)\s*([0-9][\d\.,]*)"
)
CURRENCY_PAT_DONG = re.compile(
    r"[‚Ç´ƒë]\s*([0-9][\d\.,]*)"
)

def _normalize_currency_text(text: str) -> str:
    text = CURRENCY_PAT_DOLLAR.sub(lambda m: f"USD {m.group(1)}", text)
    text = CURRENCY_PAT_VND.sub(lambda m: f"VND {m.group(1)}", text)
    text = CURRENCY_PAT_DONG.sub(lambda m: f"VND {m.group(1)}", text)
    text = re.sub(r"\b(USD|VND)\s+([0-9])", r"\1 \2", text)
    return text



def _apply_lexical_fixes(t: str) -> str:
    """
    V√° c√°c bi·∫øn th·ªÉ OCR c·ªßa 'no-man's-land':
    - noman's-land / noman‚Äôs-land
    - noman‚Äôs ‚Äî land / noman‚Äôs - land
    - no man‚Äôs land (b·ªã t√°ch kho·∫£ng tr·∫Øng)
    - nomansland (d√≠nh li·ªÅn)
    """
    patterns = [
        r"\bnoman['‚Äô]?s?\s*(?:-|‚Äî|‚Äì)?\s*land\b",  # noman‚Äôs-land / noman‚Äôs ‚Äî land
        r"\bno\s*man['‚Äô]?\s*(?:-|‚Äî|‚Äì)?\s*land\b", # no man‚Äôs land / no-man‚Äôs land
        r"\bnomansland\b",                        # nomansland
    ]
    for pat in patterns:
        t = re.sub(pat, "no-man's-land", t, flags=re.I)
    return t




def clean_text_block(raw: str) -> str:
    t = (raw or "").replace("\u00A0", " ").replace("\t", " ").replace("\r", "")
    t = _apply_punct_map(t)
    t = _normalize_ascii_dash(t)

    t = re.sub(r" ?\| ?", " ", t)            # b·ªè '|' l·∫ª trong text
    t = _normalize_currency_text(t)          # chu·∫©n ho√° ti·ªÅn t·ªá tr∆∞·ªõc khi l·ªçc k√Ω t·ª±
    t = _soft_join_hyphen(t)
    t = _soft_join_sentences(t)

    kept = []
    badline_res = [re.compile(p, re.I) for p in TEXT_BADLINES_PATTERNS]
    for line in t.splitlines():
        s = line.strip()
        drop = any(p.search(s) for p in badline_res)
        if not drop:
            kept.append(line)
    t = "\n".join(kept)

    t = re.sub(WEIRD_CHARS, "", t)
    t = re.sub(r"[ \t]{2,}", " ", t)         # chu·∫©n ho√° kho·∫£ng tr·∫Øng
    t = _apply_lexical_fixes(t)

    # ch√®n kho·∫£ng tr·∫Øng sau d·∫•u ':' n·∫øu tr∆∞·ªõc ƒë√≥ KH√îNG ph·∫£i l√† s·ªë (tr√°nh 12:30), v√† sau ƒë√≥ l√† ch·ªØ/s·ªë
    t = re.sub(r"(?<!\d):(?=[A-Za-z√Ä-·ªπ0-9])", ": ", t)

    # collapse 3+ newline ‚Üí 2 newline
    t = re.sub(r"\n{3,}", "\n\n", t.strip())
    return t.strip()


# ============== PH√ÇN KH·ªêI + L·ªåC THEO TRANG ==============
# A1 marker m·∫´u: "### [PDF page 26] [TEXT]" ho·∫∑c "### [PDF page 26] [TABLE 1]"
MARKER_RE   = re.compile(r"^###\s*\[(?P<src>[^\]]+)\]\s*\[(?P<kind>TEXT|TABLE[^\]]*)\]\s*$", flags=re.I)
PDF_PAGE_RE = re.compile(r"(?i)^PDF\s+page\s+(\d+)$")

def parse_blocks_with_page(mixed_text: str) -> List[Dict]:
    blocks = []
    cur = None
    for line in (mixed_text or "").splitlines():
        m = MARKER_RE.match(line)
        if m:
            # flush block c≈©
            if cur and cur.get("buf"):
                cur["content"] = "\n".join(cur["buf"]).strip()
                blocks.append({k: v for k, v in cur.items() if k != "buf"})
            # m·ªü block m·ªõi
            src = (m.group("src") or "").strip()
            kind = (m.group("kind") or "").strip().upper()
            page = None
            pm = PDF_PAGE_RE.match(src)
            if pm:
                try:
                    page = int(pm.group(1))
                except Exception:
                    page = None
            cur = {
                "type": ("text" if kind.startswith("TEXT") else "table"),
                "page": page,
                "src": src,
                "kind": kind,
                "buf": []
            }
        else:
            if cur is None:
                # n·∫øu tr∆∞·ªõc ƒë√≥ ch∆∞a c√≥ marker h·ª£p l·ªá th√¨ b·ªè qua r√°c header
                continue
            cur["buf"].append(line)
    if cur and cur.get("buf"):
        cur["content"] = "\n".join(cur["buf"]).strip()
        blocks.append({k: v for k, v in cur.items() if k != "buf"})
    return blocks

def filter_blocks_by_page(blocks: List[Dict], start: Optional[int], end: Optional[int]) -> List[Dict]:
    if not start and not end:
        return blocks
    keep = []
    for b in blocks:
        p = b.get("page")
        if p is None:
            # C√≥ filter trang th√¨ ch·ªâ gi·ªØ c√°c block PDF c√≥ s·ªë trang
            continue
        if start and p < start:
            continue
        if end and p > end:
            continue
        keep.append(b)
    return keep

# ============== GHI VECTOR.JSONL ==============
def append_vector_jsonl(path: Path, content: str, meta: dict):
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps({"content": content, "metadata": meta}, ensure_ascii=False) + "\n")

# ============== X·ª¨ L√ù 1 C·∫∂P FILE (T·ª™ IN_ROOT ‚Üí OUT_ROOT MIRROR) ==============
def _purge_outputs(out_txt: Path, out_meta: Path, out_vec: Path):
    """Xo√° s·∫°ch 3 file ƒë√≠ch n·∫øu t·ªìn t·∫°i (ch·∫ø ƒë·ªô Y=purge)."""
    for p in (out_txt, out_meta, out_vec):
        try:
            if p.exists():
                p.unlink()
        except Exception:
            pass

def process_pair(
    in_root: Path,
    out_root: Path,
    txt_path: Path,
    meta_path: Path,
    write_mode: str,
    page_start: Optional[int],
    page_end: Optional[int]
):
    # T√≠nh ƒë∆∞·ªùng d·∫´n mirror
    rel_dir  = txt_path.parent.relative_to(in_root)        # th∆∞ m·ª•c con so v·ªõi IN_ROOT
    base_raw = txt_path.name.replace("_text.txt", "")      # t√™n g·ªëc (kh√¥ng ƒëu√¥i)
    out_dir  = out_root / rel_dir
    ensure_dir(out_dir)

    out_txt  = out_dir / f"{base_raw}_text.txt"
    out_meta = out_dir / f"{base_raw}_meta.json"
    out_vec  = out_dir / f"{base_raw}_vector.jsonl"

    # skip n·∫øu ƒë√£ c√≥
    if write_mode == "skip" and out_txt.exists() and out_meta.exists() and out_vec.exists():
        print(f"‚è≠Ô∏è  Skip (ƒë√£ c√≥ ƒë·ªß 3 file): {out_txt}")
        return

    # purge: x√≥a c·∫£ 3 file tr∆∞·ªõc khi x·ª≠ l√Ω
    if write_mode == "purge":
        _purge_outputs(out_txt, out_meta, out_vec)

    # === ƒë·ªçc input ===
    raw_text = txt_path.read_text(encoding="utf-8", errors="ignore")
    try:
        meta_in = json.loads(meta_path.read_text(encoding="utf-8", errors="ignore") or "{}")
    except Exception:
        meta_in = {}

    # === t√°ch block theo marker A1 v√† l·ªçc theo trang (n·∫øu c√≥) ===
    blocks_all = parse_blocks_with_page(raw_text)
    blocks     = filter_blocks_by_page(blocks_all, page_start, page_end)

    cleaned_parts: List[str] = []
    kept_count = 0

    # CH·ªà x·ª≠ l√Ω TEXT; m·ªçi TABLE ƒë·ªÅu b·ªè qua ·ªü b∆∞·ªõc n√†y
    for b in blocks:
        if b.get("type") != "text":
            continue
        c = (b.get("content") or "").strip()
        if not c:
            continue
        cleaned = clean_text_block(c)
        if cleaned:
            kept_count += 1
            cleaned_parts += [f"### [TEXT CLEAN] [SRC={b.get('src','')}]", cleaned]
            append_vector_jsonl(
                out_vec,
                cleaned,
                {**meta_in, "content_type": "TEXT", "stage": "final_clean",
                 "page": b.get("page"), "src": b.get("src")}
            )

    # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ marker h·ª£p l·ªá (hi·∫øm) ‚Üí coi l√† TEXT nguy√™n kh·ªëi
    if not blocks_all:
        cleaned = clean_text_block(raw_text)
        if cleaned:
            kept_count += 1
            cleaned_parts = ["### [TEXT CLEAN] [SRC=UNKNOWN]", cleaned]
            append_vector_jsonl(out_vec, cleaned, {**meta_in, "content_type":"TEXT", "stage":"final_clean"})

    final_text = "\n".join(cleaned_parts).strip()

    # Chu·∫©n b·ªã META out
    meta_out = dict(meta_in)
    meta_out["final_clean_sha1"] = sha1_of_text(final_text)
    if page_start or page_end:
        meta_out["pages_filtered"] = {"start": page_start, "end": page_end}
    meta_out["clean_blocks_kept"] = kept_count
    meta_out["a2_mode"] = write_mode

    # === Ghi text/meta theo ch·∫ø ƒë·ªô ===
    # V·ªõi 'append': vector ƒë√£ append ·ªü tr√™n; text/meta ghi l·∫°i ƒë·ªÉ ƒë·ªìng b·ªô n·ªôi dung m·ªõi.
    if write_mode in ("purge", "append"):
        out_txt.write_text(final_text, encoding="utf-8")
        out_meta.write_text(json.dumps(meta_out, ensure_ascii=False, indent=2), encoding="utf-8")
    else:
        # skip-mode: ch·ªâ ghi n·∫øu thi·∫øu
        if not out_txt.exists():
            out_txt.write_text(final_text, encoding="utf-8")
        if not out_meta.exists():
            out_meta.write_text(json.dumps(meta_out, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"‚úÖ Wrote: {out_txt.name}, {out_meta.name}, + vector.jsonl ({write_mode}) ‚Üí {out_dir}")

# ============== CLI ==============
def build_argparser():
    p = argparse.ArgumentParser(description="Clean TEXT tr∆∞·ªõc vectorize (mirror in‚Üíout)")
    p.add_argument("--in-root",  default=IN_ROOT_DEFAULT,  help="Th∆∞ m·ª•c ch·ª©a ƒë·∫ßu ra A1 (*_text.txt + *_meta.json)")
    p.add_argument("--out-root", default=OUT_ROOT_DEFAULT, help="Th∆∞ m·ª•c ƒë√≠ch A2 (s·∫Ω mirror c√¢y th∆∞ m·ª•c)")
    p.add_argument("--start", type=int, default=None, help="[PDF] Trang b·∫Øt ƒë·∫ßu (1-based). Ch·ªâ gi·ªØ block 'PDF page X' trong kho·∫£ng.")
    p.add_argument("--end",   type=int, default=None, help="[PDF] Trang k·∫øt th√∫c (1-based, inclusive).")
    return p

def main():
    args = build_argparser().parse_args()
    in_root  = Path(args.in_root)
    out_root = Path(args.out_root)
    ensure_dir(out_root)

    write_mode = ask_write_mode_once()   # purge / skip / append
    pairs = find_pairs(in_root)
    if not pairs:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·∫∑p *_text.txt + *_meta.json n√†o trong IN_ROOT.")
        return

    total = 0
    for txt_path, meta_path in pairs:
        total += 1
        process_pair(in_root, out_root, txt_path, meta_path, write_mode, args.start, args.end)

    print(f"\nüéØ Ho√†n t·∫•t A2 (TEXT ONLY): {total} file(s). Output ·ªü: {out_root}")

if __name__ == "__main__":
    main()
