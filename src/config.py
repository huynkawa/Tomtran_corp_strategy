from typing import Any
import os
import src.env  # Ä‘áº£m báº£o biáº¿n mÃ´i trÆ°á»ng Ä‘Ã£ Ä‘Æ°á»£c náº¡p

# --- Chroma persist dir ---
PERSIST_DIR = os.getenv("VECTOR_STORE_DIR", "vector_store")

# --- Model embedding máº·c Ä‘á»‹nh ---
# CÃ³ thá»ƒ Ä‘áº·t trong .env.active:
#   EMBED_MODEL=text-embedding-3-small
#   EMBED_MODEL=text-embedding-3-large
#   EMBED_MODEL=BAAI/bge-m3
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-large")

# --- Model re-ranker máº·c Ä‘á»‹nh ---
# CÃ³ thá»ƒ Ä‘áº·t trong .env.active:
#   RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
#   RERANK_MODEL=BAAI/bge-reranker-large
RERANK_MODEL = os.getenv("RERANK_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")


def make_embeddings() -> Any:
    """
    Táº¡o embeddings theo cáº¥u hÃ¬nh EMBED_MODEL.
    - Náº¿u EMBED_MODEL báº¯t Ä‘áº§u báº±ng "text-embedding" -> dÃ¹ng OpenAI
    - NgÆ°á»£c láº¡i -> giáº£ Ä‘á»‹nh lÃ  HuggingFace model
    """
    batch = int(os.getenv("EMBED_BATCH_SIZE", "16"))

    if EMBED_MODEL.startswith("text-embedding"):
        from langchain_openai import OpenAIEmbeddings
        print(f"[config] ðŸš€ Using OpenAI embeddings: {EMBED_MODEL} | batch={batch}")
        return OpenAIEmbeddings(model=EMBED_MODEL, chunk_size=batch)
    else:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        print(f"[config] ðŸš€ Using HuggingFace embeddings: {EMBED_MODEL}")
        return HuggingFaceEmbeddings(model_name=EMBED_MODEL)
