import os, shutil, io, json
from pathlib import Path
from typing import List

import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import camelot
import pandas as pd

from dotenv import load_dotenv
from langchain.schema import Document
from langchain_community.document_loaders import (
    DirectoryLoader, PyPDFLoader, Docx2txtLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

from src.config import make_embeddings
from langchain_chroma import Chroma
from tqdm import tqdm   # ‚úÖ progress bar

# === Load env ===
load_dotenv()
DATA_DIR = os.getenv("DATA_DIR", "data")
INPUT_DIR = os.getenv("INPUT_DIR", "inputs")
OUT_DIR = os.getenv("OUTPUT_DIR", "outputs")
VECTOR_DIR = os.getenv("VECTOR_DIR", "vector_store")
OCR_DIR = os.path.join(OUT_DIR, "ocr_texts")
OCR_PDF_DIR = os.path.join(OUT_DIR, "ocr_pdfs")
CSV_DIR = os.path.join(OUT_DIR, "tables")

os.makedirs(OCR_DIR, exist_ok=True)
os.makedirs(OCR_PDF_DIR, exist_ok=True)
os.makedirs(CSV_DIR, exist_ok=True)


# === H√†m OCR PDF (PyMuPDF + pytesseract) ===
def ensure_ocr_pdf(file_path: str) -> str:
    try:
        pdf_docs = PyPDFLoader(file_path).load()

        if all(len(d.page_content.strip()) == 0 for d in pdf_docs):
            print(f"[Fallback OCR] {file_path} ‚Üí D√πng PyMuPDF + pytesseract")

            text_out = os.path.join(OCR_DIR, f"{Path(file_path).stem}.txt")
            with fitz.open(file_path) as doc, open(text_out, "w", encoding="utf-8") as out:
                for page_num in range(len(doc)):
                    pix = doc[page_num].get_pixmap(dpi=300)
                    img = Image.open(io.BytesIO(pix.tobytes("png")))
                    text = pytesseract.image_to_string(img, lang="vie+eng")
                    out.write(f"\n--- Page {page_num+1} ---\n{text}\n")
            return file_path

    except Exception as e:
        print(f"[OCR l·ªói chung] {file_path} ‚Üí {e}")

    return file_path


# === H√†m chuy·ªÉn b·∫£ng th√†nh text d·ªÖ hi·ªÉu ===
def table_to_text(df: pd.DataFrame, source: str, table_name: str = "") -> str:
    lines = []
    columns = [str(c).strip() for c in df.columns]
    if table_name:
        lines.append(f"B·∫£ng: {table_name}")
    lines.append("Ti√™u ƒë·ªÅ c·ªôt: " + " | ".join(columns))
    for idx, row in df.iterrows():
        row_str = ", ".join(f"{columns[i]} = {str(val).strip()}" for i, val in enumerate(row))
        lines.append(f"D√≤ng {idx+1}: {row_str}")
    return "\n".join(lines)


# === Tr√≠ch xu·∫•t b·∫£ng t·ª´ PDF ===
def extract_tables(file_path: str) -> List[Document]:
    docs = []
    try:
        tables = camelot.read_pdf(file_path, pages="all", flavor="stream")
        for i, table in enumerate(tables):
            df = table.df
            table_text = table_to_text(df, file_path, f"PDF Table {i+1}")
            if table_text.strip():
                docs.append(Document(
                    page_content=table_text,
                    metadata={"source": file_path, "page": i+1, "type": "table"}
                ))
                csv_out = os.path.join(CSV_DIR, f"{Path(file_path).stem}_table_{i+1}.csv")
                df.to_csv(csv_out, index=False, encoding="utf-8-sig")
                print(f"[CSV Export] {csv_out}")
    except Exception as e:
        print(f"[Camelot l·ªói] {file_path} ‚Üí {e}")
    return docs


# === Chunk t√†i li·ªáu ===
def chunk_documents(docs: List[Document], chunk_size=900, chunk_overlap=120) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    all_chunks: List[Document] = []
    print("[ingest] Chunking...")
    for doc in docs:
        if doc.metadata.get("type") == "table":
            all_chunks.append(doc)
        else:
            chunks = splitter.split_documents([doc])
            all_chunks.extend(chunks)
    return all_chunks


# === Load documents t·ªïng h·ª£p ===
def load_documents(dir_path: str) -> List[Document]:
    docs: List[Document] = []
    if not os.path.isdir(dir_path):
        return docs
    for file in Path(dir_path).rglob("*"):
        ext = file.suffix.lower()
        if ext == ".pdf":
            real_file = ensure_ocr_pdf(str(file))
            docs.extend(extract_tables(real_file))
            try:
                docs.extend(PyPDFLoader(real_file).load())
            except:
                pass
        elif ext in [".docx"]:
            docs.extend(DirectoryLoader(dir_path, glob="**/*.docx", loader_cls=Docx2txtLoader).load())
        elif ext in [".xls", ".xlsx", ".csv"]:
            try:
                if ext == ".csv":
                    df = pd.read_csv(file)
                    docs.append(Document(
                        page_content=table_to_text(df, str(file), f"CSV {Path(file).stem}"),
                        metadata={"source": str(file), "type": "table"}
                    ))
                else:
                    xl = pd.ExcelFile(file)
                    for sheet in xl.sheet_names:
                        df = xl.parse(sheet)
                        csv_out = os.path.join(CSV_DIR, f"{Path(file).stem}_{sheet}.csv")
                        df.to_csv(csv_out, index=False, encoding="utf-8-sig")
                        print(f"[Excel Export] {csv_out}")
                        docs.append(Document(
                            page_content=table_to_text(df, str(file), f"Sheet {sheet}"),
                            metadata={"source": str(file), "sheet": sheet, "type": "table"}
                        ))
            except Exception as e:
                print(f"[Excel/CSV l·ªói] {file} ‚Üí {e}")
        elif ext in [".txt", ".md"]:
            docs_txt = TextLoader(str(file), encoding="utf-8").load()
            base = str(file).replace("_text.txt", "")
            meta_file = base + "_meta.json"
            if os.path.exists(meta_file):
                with open(meta_file, "r", encoding="utf-8") as f:
                    meta = json.load(f)
                for d in docs_txt:
                    d.metadata.update(meta)
            docs.extend(docs_txt)
    return docs


# === Main ===
if __name__ == "__main__":
    sub_vector_dir = os.path.join(VECTOR_DIR, "raw_clean_data")

    print("\n======================================")
    print("‚öôÔ∏è  L·ª±a ch·ªçn ch·∫ø ƒë·ªô build d·ªØ li·ªáu")
    print("   - G√µ 'r' r·ªìi Enter ‚Üí Xo√° d·ªØ li·ªáu c≈© v√† build l·∫°i t·ª´ ƒë·∫ßu")
    print("   - G√µ 'c' r·ªìi Enter ‚Üí Gi·ªØ nguy√™n d·ªØ li·ªáu c≈© (kh√¥ng build)")
    print("   - Nh·∫•n Enter (b·ªè tr·ªëng) ‚Üí m·∫∑c ƒë·ªãnh = Continue (c)")
    print("======================================")

    choice = input("üëâ B·∫°n ch·ªçn [R/c]: ").strip().lower()

    if choice == "" or choice == "c":
        print("‚è≠Ô∏è Ti·∫øp t·ª•c gi·ªØ d·ªØ li·ªáu c≈©. Kh√¥ng build l·∫°i.")
        exit(0)
    elif choice == "r":
        if os.path.exists(sub_vector_dir):
            shutil.rmtree(sub_vector_dir)
            print(f"üóëÔ∏è ƒê√£ xo√° vector c≈©: {sub_vector_dir}")
        if os.path.exists(CSV_DIR):
            shutil.rmtree(CSV_DIR)
            os.makedirs(CSV_DIR, exist_ok=True)
            print(f"üóëÔ∏è ƒê√£ xo√° CSV c≈©: {CSV_DIR}")
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Tho√°t.")
        exit(1)

    print(f"\nüì• ƒêang n·∫°p d·ªØ li·ªáu t·ª´ {DATA_DIR} v√† {INPUT_DIR}")

    sources = []
    for p in (DATA_DIR, INPUT_DIR):
        sources += load_documents(p)

    chunks = chunk_documents(sources)
    print(f"[ingest] T·ªïng s·ªë chunks: {len(chunks)}")

    os.makedirs(sub_vector_dir, exist_ok=True)

    embeddings = make_embeddings()
    vectordb = Chroma.from_documents(
        documents=tqdm(chunks, desc="[embedding raw_data]", unit="chunk"),
        embedding=embeddings,
        persist_directory=sub_vector_dir
    )

    print("\n=== T√ìM T·∫ÆT ===")
    print(f"üìÑ T·ªïng document: {len(sources)}")
    print(f"üîñ T·ªïng chunks: {len(chunks)}")
    print(f"üìä B·∫£ng (gi·ªØ nguy√™n): {len([d for d in chunks if d.metadata.get('type')=='table'])}")
    print(f"üìÇ Vector l∆∞u t·∫°i: {sub_vector_dir}")
    print(f"üìÇ CSV b·∫£ng l∆∞u t·∫°i: {CSV_DIR}")
