# app.py (root level)
import os
import streamlit as st
import src.env  # sáº½ tá»± náº¡p OPENAI_API_KEY
from src.rag_chain import rag_answer

# --- DÃ¹ng sqlite3 máº·c Ä‘á»‹nh ---
import sqlite3

# --- LuÃ´n Ã©p dÃ¹ng OpenAI client ---
from openai import OpenAI

api_key = os.getenv("OPENAI_API_KEY")
if not api_key or not api_key.strip():
    raise RuntimeError("âŒ OPENAI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh trong .env.active")

client = OpenAI(api_key=api_key)
print("[app] ğŸš€ OpenAI client initialized.")

from src.prompt_loader import load_prompts, render_system_prompt
from langchain_chroma import Chroma
from src.config import make_embeddings, EMBED_MODEL
from langchain_openai import ChatOpenAI

# ğŸš€ Cáº¥u hÃ¬nh trang
st.set_page_config(page_title="TOMTRANCHATBOT", layout="wide")
st.title("TOMTRANCHATBOT")

# Load prompts, khÃ³a luÃ´n vÃ o rag
cfg = load_prompts("prompts/prompts.yaml")
selected_key = "rag"

# Sidebar controls
temperature = st.sidebar.slider("Temperature", 0.0, 1.2, 0.3, 0.1)
top_p = st.sidebar.slider("Top_p", 0.1, 1.0, 1.0, 0.1)
fallback_general = st.sidebar.checkbox("Fallback GPT náº¿u khÃ´ng cÃ³ tÃ i liá»‡u phÃ¹ há»£p", value=True)
K = st.sidebar.slider("Sá»‘ Ä‘oáº¡n context (k)", 1, 12, 4, 1)
MIN_RELEVANCE = st.sidebar.slider(
    "NgÆ°á»¡ng Ä‘iá»ƒm liÃªn quan tá»‘i thiá»ƒu (0â€“1, cao = cháº·t)", 0.0, 1.0, 0.30, 0.05
)
debug_mode = st.sidebar.checkbox("ğŸ”§ Debug Mode", value=False)

# System prompt
system_prompt = render_system_prompt(cfg, selected_key)
with st.expander("ğŸ”§ System prompt Ä‘ang dÃ¹ng", expanded=False):
    st.code(system_prompt, language="markdown")

@st.cache_resource
def get_vectordb():
    vector_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
    return Chroma(persist_directory=vector_dir, embedding_function=make_embeddings())

vectordb = get_vectordb()
llm = ChatOpenAI(model="gpt-4o-mini", temperature=temperature)

# Debug diagnostics
with st.expander("ğŸ§ª RAG diagnostics", expanded=False):
    try:
        emb = make_embeddings()
        st.write("Embedding class:", emb.__class__.__name__)
        vector_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
        st.write("Persist dir:", os.path.abspath(vector_dir))
        count = getattr(vectordb, "_collection").count()
        st.write("Vector count:", count)
    except Exception as e:
        st.warning(f"Diag error: {e}")

# --- Session state ---
if "history" not in st.session_state:
    st.session_state.history = []

# --- Input ---
user_msg = st.chat_input("Nháº­p cÃ¢u há»iâ€¦")
if user_msg:
    st.session_state.history.append(("user", user_msg))
    latest_query = user_msg

    result = rag_answer(
        query=latest_query,
        retriever_or_db=vectordb,   # âš¡ Æ¯u tiÃªn dÃ¹ng vectordb gá»‘c
        llm=llm,
        client=client,
        use_fallback=fallback_general,
        threshold=MIN_RELEVANCE,
        k=K,
    )

    # Decorate message theo nguá»“n
    if result["source"] == "internal":
        decorated_msg = (
            "<div style='background-color:#e8f5e9; padding:10px; border-radius:10px;'>"
            "ğŸ›ï¸ <b>Tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c ná»™i bá»™</b></div>\n\n"
            + result["answer"]
        )
    elif result["source"] == "general":
        decorated_msg = (
            "<div style='background-color:#f5f5f5; padding:10px; border-radius:10px;'>"
            "ğŸŒ <b>Tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c tá»•ng quan</b></div>\n\n"
            + result["answer"]
        )
    else:
        decorated_msg = result["answer"]

    st.session_state.history.append(("assistant", decorated_msg))

    # --- Debug Mode ---
    if debug_mode:
        with st.expander("ğŸ“‚ Context (debug)", expanded=False):
            st.markdown(result["ctx_text"] or "â€”")

        if result["docs"]:
            st.write("ğŸ” Láº¥y Ä‘Æ°á»£c", len(result["docs"]), "tÃ i liá»‡u liÃªn quan")
            for d in result["docs"]:
                st.text(f"- {d.metadata.get('source', 'unknown')}")

# --- Render history ---
for role, content in st.session_state.history:
    with st.chat_message(role):
        st.markdown(content, unsafe_allow_html=True)

# --- Footer ---
st.markdown(
    f"<hr><div style='text-align:center; color:gray; font-size:0.9em'>"
    f"â˜ï¸ Embedding: OpenAI â€“ {EMBED_MODEL}</div>",
    unsafe_allow_html=True,
)
