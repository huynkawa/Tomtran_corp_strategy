# app.py (root level)
import os
import streamlit as st
import src.env  # s·∫Ω t·ª± n·∫°p OPENAI_API_KEY
from src.rag_chain import rag_answer

# --- D√πng sqlite3 m·∫∑c ƒë·ªãnh ---
import sqlite3

from src.prompt_loader import load_prompts, render_system_prompt
from langchain_chroma import Chroma
from src.config import make_embeddings, EMBED_MODEL

# ‚úÖ Th√™m import ƒë·ªÉ t·∫£i t·ª´ Google Drive
import gdown
import zipfile

# üöÄ C·∫•u h√¨nh trang
st.set_page_config(page_title="TOMTRANCHATBOT", layout="wide")
st.title("TOMTRANCHATBOT")

# Load prompts, kh√≥a lu√¥n v√†o rag
cfg = load_prompts("prompts/prompts.yaml")
selected_key = "rag"

# Sidebar controls
temperature = st.sidebar.slider("Temperature", 0.0, 1.2, 0.3, 0.1)
top_p = st.sidebar.slider("Top_p", 0.1, 1.0, 1.0, 0.1)
fallback_general = st.sidebar.checkbox("Fallback GPT n·∫øu kh√¥ng c√≥ t√†i li·ªáu ph√π h·ª£p", value=True)
K = st.sidebar.slider("S·ªë ƒëo·∫°n context (k)", 1, 12, 4, 1)
MIN_RELEVANCE = st.sidebar.slider(
    "Ng∆∞·ª°ng ƒëi·ªÉm li√™n quan t·ªëi thi·ªÉu (0‚Äì1, cao = ch·∫∑t)", 0.0, 1.0, 0.30, 0.05
)
debug_mode = st.sidebar.checkbox("üîß Debug Mode", value=False)

# System prompt
system_prompt = render_system_prompt(cfg, selected_key)
with st.expander("üîß System prompt ƒëang d√πng", expanded=False):
    st.code(system_prompt, language="markdown")

# === Google Drive Downloader ===
def ensure_vectorstore_from_gdrive():
    VECTOR_DIR = os.getenv("VECTOR_STORE_DIR", "vector_store")
    ZIP_PATH = "vector_store.zip"

    # üîë Thay ID n√†y b·∫±ng ID file th·∫≠t c·ªßa b·∫°n tr√™n Google Drive
    GOOGLE_DRIVE_FILE_ID = "YOUR_FILE_ID_HERE"

    if not os.path.exists(VECTOR_DIR):
        st.warning("‚ö†Ô∏è vector_store ch∆∞a c√≥, ƒëang t·∫£i t·ª´ Google Drive...")
        url = f"https://drive.google.com/uc?id={GOOGLE_DRIVE_FILE_ID}"
        gdown.download(url, ZIP_PATH, quiet=False)

        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
            zip_ref.extractall(".")
        st.success("‚úÖ ƒê√£ t·∫£i v√† gi·∫£i n√©n vector_store!")

# G·ªçi ƒë·∫£m b·∫£o vector_store c√≥ s·∫µn tr∆∞·ªõc khi load
ensure_vectorstore_from_gdrive()

# === VectorDB Loader ===
@st.cache_resource
def get_vectordb():
    base_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
    cleaned_root = os.path.join(base_dir, "cleaned_scan_data")

    dbs = []
    if os.path.exists(cleaned_root):
        for sub in os.listdir(cleaned_root):
            sub_path = os.path.join(cleaned_root, sub)
            if os.path.isdir(sub_path):
                try:
                    db = Chroma(
                        persist_directory=sub_path,
                        embedding_function=make_embeddings()
                    )
                    dbs.append(db)
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi load {sub_path}: {e}")

    # N·∫øu kh√¥ng c√≥ sub-db n√†o th√¨ fallback
    if not dbs:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y sub-vectorstore n√†o, d√πng vector_store g·ªëc.")
        return Chroma(persist_directory=base_dir, embedding_function=make_embeddings())

    # H·ª£p nh·∫•t nhi·ªÅu DB th√†nh 1 retriever
    class UnionRetriever:
        def similarity_search_with_relevance_scores(self, query, k=6):
            results = []
            for db in dbs:
                try:
                    results.extend(db.similarity_search_with_relevance_scores(query, k=k))
                except Exception as e:
                    print("‚ö†Ô∏è Query l·ªói:", e)
            results = sorted(results, key=lambda x: x[1] if x[1] else 0, reverse=True)
            return results[:k]

    return UnionRetriever()

vectordb = get_vectordb()

# === Debug diagnostics ===
with st.expander("üß™ RAG diagnostics", expanded=False):
    try:
        emb = make_embeddings()
        st.write("Embedding class:", emb.__class__.__name__)

        base_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
        cleaned_root = os.path.join(base_dir, "cleaned_scan_data")

        if os.path.exists(cleaned_root):
            st.write("Root dir:", os.path.abspath(cleaned_root))
            total = 0
            for sub in os.listdir(cleaned_root):
                sub_path = os.path.join(cleaned_root, sub)
                if os.path.isdir(sub_path):
                    try:
                        db = Chroma(
                            persist_directory=sub_path,
                            embedding_function=make_embeddings()
                        )
                        count = getattr(db, "_collection").count()
                        total += count
                        st.write(f"üìÇ {sub}: {count} vectors")
                    except Exception as e:
                        st.write(f"‚ö†Ô∏è {sub} l·ªói: {e}")
            st.write(f"üî¢ T·ªïng c·ªông: {total} vectors")
        else:
            st.write("Persist dir:", os.path.abspath(base_dir))
            count = getattr(vectordb, "_collection").count()
            st.write("Vector count:", count)

    except Exception as e:
        st.warning(f"Diag error: {e}")

# --- Session state ---
if "history" not in st.session_state:
    st.session_state.history = []

# --- Input ---
user_msg = st.chat_input("Nh·∫≠p c√¢u h·ªèi‚Ä¶")
if user_msg:
    st.session_state.history.append(("user", user_msg))
    latest_query = user_msg

    # ‚úÖ G·ªçi rag_answer kh√¥ng c·∫ßn llm n·ªØa
    result = rag_answer(
        query=latest_query,
        retriever_or_db=vectordb,
        use_fallback=fallback_general,
        threshold=MIN_RELEVANCE,
        k=K,
        debug=debug_mode,
    )

    # Decorate message theo ngu·ªìn
    assistant_id = os.getenv("ASSISTANT_ID", "unknown")

    if result["source"] == "internal":
        decorated_msg = (
            "<div style='background-color:#e8f5e9; padding:10px; border-radius:10px;'>"
            "üèõÔ∏è <b>Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c n·ªôi b·ªô</b> ‚Äî ü§ñ Assistant API</div>\n\n"
            + result["answer"]
        )
    elif result["source"] == "general":
        decorated_msg = (
            "<div style='background-color:#f5f5f5; padding:10px; border-radius:10px;'>"
            "üåê <b>Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng quan</b> ‚Äî ü§ñ Assistant API</div>\n\n"
            + result["answer"]
        )
    else:
        decorated_msg = "ü§ñ Assistant API\n\n" + result["answer"]



    st.session_state.history.append(("assistant", decorated_msg))

    if debug_mode:
        with st.expander("üìÇ Context (debug)", expanded=False):
            st.markdown(result["ctx_text"] or "‚Äî")

        if result["docs"]:
            st.write("üîé L·∫•y ƒë∆∞·ª£c", len(result["docs"]), "t√†i li·ªáu li√™n quan")
            for d in result["docs"]:
                st.text(f"- {d.metadata.get('source', 'unknown')}")

# --- Render history ---
for role, content in st.session_state.history:
    with st.chat_message(role):
        st.markdown(content, unsafe_allow_html=True)

# --- Footer ---
st.markdown(
    f"<hr><div style='text-align:center; color:gray; font-size:0.9em'>"
    f"‚òÅÔ∏è Embedding: OpenAI ‚Äì {EMBED_MODEL}</div>",
    unsafe_allow_html=True,
)
