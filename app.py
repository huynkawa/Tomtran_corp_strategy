# app.py (root level)
import os
import streamlit as st
import src.env  # s·∫Ω t·ª± n·∫°p OPENAI_API_KEY
import sqlite3
import gdown, zipfile
from src.rag_chain import rag_answer
from src.prompt_loader import load_prompts, render_system_prompt
from src.config import make_embeddings, EMBED_MODEL
from langchain_chroma import Chroma

# --- Giao di·ªán ChatGPT style ---
from src.ui_layout import render_ui
from src.chat_saver import save_chat

# üöÄ T·∫°o UI v√† l·∫•y c√°c tham s·ªë
user_msg, temperature, top_p, fallback_general, K, MIN_RELEVANCE, debug_mode, show_system, show_rag = render_ui("TOMTRANCHATBOT")

# === Load Prompt c·∫•u h√¨nh ===
cfg = load_prompts("prompts/prompts.yaml")
selected_key = "rag"

system_prompt = render_system_prompt(cfg, selected_key)
if show_system:  # ‚úÖ ch·ªâ hi·ªÉn th·ªã khi ng∆∞·ªùi d√πng b·∫≠t toggle
    with st.expander("üîß System prompt ƒëang d√πng", expanded=False):
        st.code(system_prompt, language="markdown")

# === Google Drive Downloader ===
def ensure_vectorstore_from_gdrive():
    VECTOR_DIR = os.getenv("VECTOR_STORE_DIR", "vector_store")
    GOOGLE_DRIVE_FOLDER_URL = "https://drive.google.com/drive/folders/1yKkKkRuYNZtSQs7biqmuMY3NGuSO9UIO?usp=sharing"

    if not os.path.exists(VECTOR_DIR):
        st.warning("‚ö†Ô∏è vector_store ch∆∞a c√≥, ƒëang t·∫£i t·ª´ Google Drive (folder)...")
        try:
            gdown.download_folder(GOOGLE_DRIVE_FOLDER_URL, output=VECTOR_DIR, quiet=False, use_cookies=False)
            st.success("‚úÖ ƒê√£ t·∫£i folder vector_store t·ª´ Google Drive!")
        except Exception as e:
            st.error(f"‚ùå L·ªói khi t·∫£i vector_store: {e}")


# G·ªçi ƒë·∫£m b·∫£o vector_store c√≥ s·∫µn
ensure_vectorstore_from_gdrive()


# === VectorDB Loader ===
@st.cache_resource
def get_vectordb():
    base_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
    cleaned_root = os.path.join(base_dir, "cleaned_scan_data")

    dbs = []
    if os.path.exists(cleaned_root):
        for sub in os.listdir(cleaned_root):
            sub_path = os.path.join(cleaned_root, sub)
            if os.path.isdir(sub_path):
                try:
                    db = Chroma(
                        persist_directory=sub_path,
                        embedding_function=make_embeddings()
                    )
                    dbs.append(db)
                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi load {sub_path}: {e}")

    if not dbs:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y sub-vectorstore n√†o, d√πng vector_store g·ªëc.")
        return Chroma(persist_directory=base_dir, embedding_function=make_embeddings())

    class UnionRetriever:
        def similarity_search_with_relevance_scores(self, query, k=6):
            results = []
            for db in dbs:
                try:
                    results.extend(db.similarity_search_with_relevance_scores(query, k=k))
                except Exception as e:
                    print("‚ö†Ô∏è Query l·ªói:", e)
            results = sorted(results, key=lambda x: x[1] if x[1] else 0, reverse=True)
            return results[:k]

    return UnionRetriever()


vectordb = get_vectordb()


# === RAG diagnostics ===
if show_rag:  # ‚úÖ ch·ªâ hi·ªÉn th·ªã khi ng∆∞·ªùi d√πng b·∫≠t toggle
    with st.expander("üß™ RAG diagnostics", expanded=False):
        try:
            emb = make_embeddings()
            st.write("Embedding class:", emb.__class__.__name__)

            base_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
            cleaned_root = os.path.join(base_dir, "cleaned_scan_data")

            if os.path.exists(cleaned_root):
                st.write("Root dir:", os.path.abspath(cleaned_root))
                total = 0
                for sub in os.listdir(cleaned_root):
                    sub_path = os.path.join(cleaned_root, sub)
                    if os.path.isdir(sub_path):
                        try:
                            db = Chroma(
                                persist_directory=sub_path,
                                embedding_function=make_embeddings()
                            )
                            count = getattr(db, "_collection").count()
                            total += count
                            st.write(f"üìÇ {sub}: {count} vectors")
                        except Exception as e:
                            st.write(f"‚ö†Ô∏è {sub} l·ªói: {e}")
                st.write(f"üî¢ T·ªïng c·ªông: {total} vectors")
            else:
                st.write("Persist dir:", os.path.abspath(base_dir))
                count = getattr(vectordb, "_collection").count()
                st.write("Vector count:", count)
        except Exception as e:
            st.warning(f"Diag error: {e}")


# --- SESSION STATE cho nhi·ªÅu ƒëo·∫°n chat ---
if "chat_histories" not in st.session_state:
    st.session_state.chat_histories = {}

# L·∫•y ƒëo·∫°n chat hi·ªán t·∫°i (t·∫°o m·∫∑c ƒë·ªãnh n·∫øu ch∆∞a c√≥)
current_chat = st.session_state.get("current_chat", "Chat #1")
if current_chat not in st.session_state.chat_histories:
    st.session_state.chat_histories[current_chat] = []

# --- X·ª¨ L√ù INPUT CHAT ---
# --- X·ª¨ L√ù INPUT CHAT ---
if user_msg:
    # ‚û§ L∆∞u tin nh·∫Øn ng∆∞·ªùi d√πng v√†o b·ªô nh·ªõ
    st.session_state.chat_histories[current_chat].append(("user", user_msg))
    latest_query = user_msg

    # ‚û§ G·ªçi RAG pipeline ƒë·ªÉ t√¨m c√¢u tr·∫£ l·ªùi
    try:
        result = rag_answer(
            query=latest_query,
            retriever_or_db=vectordb,
            use_fallback=fallback_general,
            threshold=MIN_RELEVANCE,
            k=K,
            debug=debug_mode,
        )

        # ‚û§ L·∫•y n·ªôi dung tr·∫£ l·ªùi
        if isinstance(result, dict):
            answer = result.get("answer", "Kh√¥ng c√≥ ph·∫£n h·ªìi.")
        else:
            answer = str(result) if result else "Kh√¥ng c√≥ ph·∫£n h·ªìi."

        # --- Trang tr√≠ ph·∫£n h·ªìi bot ---
        decorated_msg = "ü§ñ Assistant API\n\n"
        if isinstance(result, dict):
            source = result.get("source", "unknown")
            if source == "internal":
                decorated_msg = answer
            elif source == "general":
                decorated_msg = (
                    "<div style='background-color:#f5f5f5; padding:10px; border-radius:10px;'>"
                    "üåê <b>Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng quan</b> ‚Äî ü§ñ Assistant API</div>\n\n"
                    + answer
                )
            else:
                decorated_msg += answer
        else:
            decorated_msg += str(result) if result else "Kh√¥ng c√≥ k·∫øt qu·∫£ tr·∫£ v·ªÅ."

        # ‚û§ L∆∞u ph·∫£n h·ªìi bot v√†o session (ch·ªâ 1 l·∫ßn duy nh·∫•t)
        st.session_state.chat_histories[current_chat].append(("bot", decorated_msg))

    except Exception as e:
        # ‚ö†Ô∏è B·∫Øt l·ªói v√† hi·ªÉn th·ªã ra giao di·ªán
        st.session_state.chat_histories[current_chat].append(
            ("bot", f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω truy v·∫•n: {e}")
        )

    # üíæ L∆∞u h·ªôi tho·∫°i ra file JSON
    try:
        save_chat(current_chat, st.session_state.chat_histories[current_chat])
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u h·ªôi tho·∫°i: {e}")

    # üîÑ L√†m m·ªõi l·∫°i giao di·ªán ƒë·ªÉ hi·ªÉn th·ªã tin nh·∫Øn m·ªõi
    st.rerun()

# --- FOOTER ---
st.markdown(
    f"<hr><div style='text-align:center; color:gray; font-size:0.9em'>"
    f"‚òÅÔ∏è Embedding: OpenAI ‚Äì {EMBED_MODEL}</div>",
    unsafe_allow_html=True,
)
