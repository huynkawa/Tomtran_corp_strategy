# app.py (root level)
import os
import streamlit as st
import src.env  # sáº½ tá»± náº¡p OPENAI_API_KEY
from src.rag_chain import rag_answer

# --- DÃ¹ng sqlite3 máº·c Ä‘á»‹nh ---
import sqlite3

from src.prompt_loader import load_prompts, render_system_prompt
from langchain_chroma import Chroma
from src.config import make_embeddings, EMBED_MODEL

# âœ… ThÃªm import Ä‘á»ƒ táº£i tá»« Google Drive
import gdown
import zipfile

# ğŸš€ Cáº¥u hÃ¬nh trang
st.set_page_config(page_title="TOMTRANCHATBOT", layout="wide")
st.title("TOMTRANCHATBOT")

# Load prompts, khÃ³a luÃ´n vÃ o rag
cfg = load_prompts("prompts/prompts.yaml")
selected_key = "rag"

# Sidebar controls
temperature = st.sidebar.slider("Temperature", 0.0, 1.2, 0.3, 0.1)
top_p = st.sidebar.slider("Top_p", 0.1, 1.0, 1.0, 0.1)
fallback_general = st.sidebar.checkbox("Fallback GPT náº¿u khÃ´ng cÃ³ tÃ i liá»‡u phÃ¹ há»£p", value=True)
K = st.sidebar.slider("Sá»‘ Ä‘oáº¡n context (k)", 1, 12, 4, 1)
MIN_RELEVANCE = st.sidebar.slider(
    "NgÆ°á»¡ng Ä‘iá»ƒm liÃªn quan tá»‘i thiá»ƒu (0â€“1, cao = cháº·t)", 0.0, 1.0, 0.30, 0.05
)
debug_mode = st.sidebar.checkbox("ğŸ”§ Debug Mode", value=False)

# System prompt
system_prompt = render_system_prompt(cfg, selected_key)
with st.expander("ğŸ”§ System prompt Ä‘ang dÃ¹ng", expanded=False):
    st.code(system_prompt, language="markdown")

# === Google Drive Downloader ===
def ensure_vectorstore_from_gdrive():
    VECTOR_DIR = os.getenv("VECTOR_STORE_DIR", "vector_store")

    # ğŸ”‘ Thay link share folder Google Drive á»Ÿ Ä‘Ã¢y
    GOOGLE_DRIVE_FOLDER_URL = "https://drive.google.com/drive/folders/1yKkKkRuYNZtSQs7biqmuMY3NGuSO9UIO?usp=sharing"

    if not os.path.exists(VECTOR_DIR):
        st.warning("âš ï¸ vector_store chÆ°a cÃ³, Ä‘ang táº£i tá»« Google Drive (folder)...")
        try:
            gdown.download_folder(GOOGLE_DRIVE_FOLDER_URL, output=VECTOR_DIR, quiet=False, use_cookies=False)
            st.success("âœ… ÄÃ£ táº£i folder vector_store tá»« Google Drive!")
        except Exception as e:
            st.error(f"âŒ Lá»—i khi táº£i vector_store: {e}")


# Gá»i Ä‘áº£m báº£o vector_store cÃ³ sáºµn trÆ°á»›c khi load
ensure_vectorstore_from_gdrive()

# === VectorDB Loader ===
@st.cache_resource
def get_vectordb():
    base_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
    cleaned_root = os.path.join(base_dir, "cleaned_scan_data")

    dbs = []
    if os.path.exists(cleaned_root):
        for sub in os.listdir(cleaned_root):
            sub_path = os.path.join(cleaned_root, sub)
            if os.path.isdir(sub_path):
                try:
                    db = Chroma(
                        persist_directory=sub_path,
                        embedding_function=make_embeddings()
                    )
                    dbs.append(db)
                except Exception as e:
                    print(f"âš ï¸ Lá»—i khi load {sub_path}: {e}")

    # Náº¿u khÃ´ng cÃ³ sub-db nÃ o thÃ¬ fallback
    if not dbs:
        st.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y sub-vectorstore nÃ o, dÃ¹ng vector_store gá»‘c.")
        return Chroma(persist_directory=base_dir, embedding_function=make_embeddings())

    # Há»£p nháº¥t nhiá»u DB thÃ nh 1 retriever
    class UnionRetriever:
        def similarity_search_with_relevance_scores(self, query, k=6):
            results = []
            for db in dbs:
                try:
                    results.extend(db.similarity_search_with_relevance_scores(query, k=k))
                except Exception as e:
                    print("âš ï¸ Query lá»—i:", e)
            results = sorted(results, key=lambda x: x[1] if x[1] else 0, reverse=True)
            return results[:k]

    return UnionRetriever()

vectordb = get_vectordb()

# === Debug diagnostics ===
with st.expander("ğŸ§ª RAG diagnostics", expanded=False):
    try:
        emb = make_embeddings()
        st.write("Embedding class:", emb.__class__.__name__)

        base_dir = os.getenv("VECTOR_STORE_DIR", "vector_store")
        cleaned_root = os.path.join(base_dir, "cleaned_scan_data")

        if os.path.exists(cleaned_root):
            st.write("Root dir:", os.path.abspath(cleaned_root))
            total = 0
            for sub in os.listdir(cleaned_root):
                sub_path = os.path.join(cleaned_root, sub)
                if os.path.isdir(sub_path):
                    try:
                        db = Chroma(
                            persist_directory=sub_path,
                            embedding_function=make_embeddings()
                        )
                        count = getattr(db, "_collection").count()
                        total += count
                        st.write(f"ğŸ“‚ {sub}: {count} vectors")
                    except Exception as e:
                        st.write(f"âš ï¸ {sub} lá»—i: {e}")
            st.write(f"ğŸ”¢ Tá»•ng cá»™ng: {total} vectors")
        else:
            st.write("Persist dir:", os.path.abspath(base_dir))
            count = getattr(vectordb, "_collection").count()
            st.write("Vector count:", count)

    except Exception as e:
        st.warning(f"Diag error: {e}")

# --- Session state ---
if "history" not in st.session_state:
    st.session_state.history = []

# --- Input ---
user_msg = st.chat_input("Nháº­p cÃ¢u há»iâ€¦")
if user_msg:
    st.session_state.history.append(("user", user_msg))
    latest_query = user_msg

    # âœ… Gá»i rag_answer khÃ´ng cáº§n llm ná»¯a
    result = rag_answer(
        query=latest_query,
        retriever_or_db=vectordb,
        use_fallback=fallback_general,
        threshold=MIN_RELEVANCE,
        k=K,
        debug=debug_mode,
    )

# Äáº£m báº£o luÃ´n cÃ³ biáº¿n result
result = locals().get("result", None)


  # Decorate message theo nguá»“n
assistant_id = os.getenv("ASSISTANT_ID", "unknown")

decorated_msg = "ğŸ¤– Assistant API\n\n"
if isinstance(result, dict):
    source = result.get("source", "unknown")
    answer = result.get("answer", "")

    if source == "internal":
        decorated_msg = (
            "<div style='background-color:#e8f5e9; padding:10px; border-radius:10px;'>"
            "ğŸ›ï¸ <b>Tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c ná»™i bá»™</b> â€” ğŸ¤– Assistant API</div>\n\n"
            + answer
        )
    elif source == "general":
        decorated_msg = (
            "<div style='background-color:#f5f5f5; padding:10px; border-radius:10px;'>"
            "ğŸŒ <b>Tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c tá»•ng quan</b> â€” ğŸ¤– Assistant API</div>\n\n"
            + answer
        )
    else:
        decorated_msg += answer
else:
    decorated_msg += str(result) if result else "KhÃ´ng cÃ³ káº¿t quáº£ tráº£ vá»."

st.session_state.history.append(("assistant", decorated_msg))

if debug_mode and isinstance(result, dict):
    with st.expander("ğŸ“‚ Context (debug)", expanded=False):
        st.markdown(result.get("ctx_text") or "â€”")

    if result.get("docs"):
        st.write("ğŸ” Láº¥y Ä‘Æ°á»£c", len(result["docs"]), "tÃ i liá»‡u liÃªn quan")
        for d in result["docs"]:
            st.text(f"- {d.metadata.get('source', 'unknown')}")

# --- Render history ---
for role, content in st.session_state.history:
    with st.chat_message(role):
        st.markdown(content, unsafe_allow_html=True)

# --- Footer ---
st.markdown(
    f"<hr><div style='text-align:center; color:gray; font-size:0.9em'>"
    f"â˜ï¸ Embedding: OpenAI â€“ {EMBED_MODEL}</div>",
    unsafe_allow_html=True,
)
